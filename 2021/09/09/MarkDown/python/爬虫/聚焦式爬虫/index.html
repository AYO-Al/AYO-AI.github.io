<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>聚焦式爬虫 | AYO</title><meta name="keywords" content="Python,开发,爬虫"><meta name="author" content="AYO,2609320892@qq.com"><meta name="copyright" content="AYO"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="聚焦式爬虫"><meta name="application-name" content="聚焦式爬虫"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="聚焦式爬虫"><meta property="og:url" content="https://www.yuki.love/2021/09/09/MarkDown/python/%E7%88%AC%E8%99%AB/%E8%81%9A%E7%84%A6%E5%BC%8F%E7%88%AC%E8%99%AB/index.html"><meta property="og:site_name" content="AYO"><meta property="og:description" content="本文介绍了Python爬虫怎样进行聚焦爬取。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://th.bing.com/th/id/R.4cb70989991c1861426358cb196b6d01?rik=83OmAlSQSFM51Q&amp;riu=http%3A%2F%2Fpic.bizhi360.com%2Fbpic%2F88%2F9588_8.jpg&amp;ehk=Vc5LFstc1coz3liGA9YtX3SOl%2BcMJTlRfHNhk%2F4%2FA0w%3D&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;_r_=a160cf65-bb3d-5a5b-fd48-403e56c22880"><meta property="article:author" content="AYO"><meta property="article:tag" content=""><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://th.bing.com/th/id/R.4cb70989991c1861426358cb196b6d01?rik=83OmAlSQSFM51Q&amp;riu=http%3A%2F%2Fpic.bizhi360.com%2Fbpic%2F88%2F9588_8.jpg&amp;ehk=Vc5LFstc1coz3liGA9YtX3SOl%2BcMJTlRfHNhk%2F4%2FA0w%3D&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;_r_=a160cf65-bb3d-5a5b-fd48-403e56c22880"><meta name="description" content="本文介绍了Python爬虫怎样进行聚焦爬取。"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="https://www.yuki.love/2021/09/09/MarkDown/python/%E7%88%AC%E8%99%AB/%E8%81%9A%E7%84%A6%E5%BC%8F%E7%88%AC%E8%99%AB/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png"},
  postHeadAiDescription: undefined,
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: {"enable":true,"default":"晚上好👋","list":[{"greeting":"晚安😴","startTime":0,"endTime":5},{"greeting":"早上好鸭👋, 祝你一天好心情！","startTime":6,"endTime":9},{"greeting":"上午好👋, 状态很好，鼓励一下～","startTime":10,"endTime":10},{"greeting":"11点多啦, 在坚持一下就吃饭啦～","startTime":11,"endTime":11},{"greeting":"午安👋, 宝贝","startTime":12,"endTime":14},{"greeting":"🌈充实的一天辛苦啦！","startTime":14,"endTime":18},{"greeting":"19点喽, 奖励一顿丰盛的大餐吧🍔。","startTime":19,"endTime":19},{"greeting":"晚上好👋, 在属于自己的时间好好放松😌~","startTime":20,"endTime":24}]},
  twikooEnvId: 'https://twikoo.yuki.love',
  commentBarrageConfig:undefined,
  root: '/',
  preloader: undefined,
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: {"skills":["🤖️ 数码科技爱好者","🔍 分享与热心帮助","🏠 智能家居小能手","🔨 设计开发一条龙","🤝 专修交互与设计","🏃 脚踏实地行动派","🧱 团队小组发动机","💢 壮汉人狠话不多"]},
  algolia: {"appId":"XMRZIUY8NL","apiKey":"187af2610e1cc500c95dc8b8f3a53d0e","indexName":"hexo","hits":{"per_page":6},"languages":{"input_placeholder":"输入关键词后按下回车查找","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: AYO","link":"链接: ","source":"来源: AYO","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'AYO',
  title: '聚焦式爬虫',
  postAI: '',
  pageFillDescription: '聚焦爬虫, 1. 数据解析, 1.1 数据解析分类, 1.2 数据解析原理, 2. 正则解析, 2.1 正则表达式, 2.2 常用正则表达式, 2.2 实战之暴走漫画, bs4解析, xpath解析, xpath实战之爬取58二手房, xpath实战之4k图片解析下载, xpath实战之全国城市名称爬取, xpath实战之图片爬取, 网站验证码, 超级鹰使用, 模拟登录, cookie操作, 代理理论, 异步爬虫, selenium模块, scrapy框架, scrapy数据解析, scrapy持久化存储, 基于终端指令, 基于管道, 全站数据爬取, 五大核心组件, 请求传参, 图片爬取值ImagesPipeline类, 中间件, CrawlSpider类, scrapy设置文件, 分布式爬虫, 关于开发者工具与页面源代码不同, 关于一次获取多个xpath数据, 关于中文乱码解决方案, 多页面爬取聚焦爬虫概述爬取页面中指定的页面内容编码流程指定发起请求获取响应数据数据解析持久化存储数据解析数据解析分类正则主要数据解析原理概述解析的局部文本内容都会在标签之间或者标签对应的属性中进行存储方法进行指定标签的定位标签或者标签对应的属性中存储的数据值进行提取解析正则解析正则表达式正则用来匹配字符串的一门表达式语言常用正则表达式实战之暴走漫画创建一个文件夹保存所有图片暴走暴走获取全部数据使用正则对图片进行解析方法获取二进制数据类型生成图片名称暴走下载成功解析只能在用数据解析原理实例化一个对象并且将页面源码数据加载到该对象中通过调用对象中相关的属性或者方法进行标签定位和数据提取环境安装如何实例化对象对象实例化将本地的文档中的数据加载到该对象中将本地的文档中的数据加载到该对象中登录第二个参数为对象使用解析器进行解析将互联网上获取的页面源码加载到该对象中与第一种差不多只要先把爬取到的全部数据存到本地文档在解析提供的用于数据解析解析的属性和方法返回中第一次出现的标签返回第一次出现的对应的标签属性定位返回所有出现的对应的标签列表某种选择器返回一个列表某种选择器某种选择器某种选择器表示一个层级空格表示多个层级获取标签中的文本数据获取一个标签里的所有文本内容获取一个标签直系的文本内容获取标签中的属性值解析最常用切最便捷高效的一种方式解析原理实例化一个的对象且需要将被解析的页面源码数据加载到该对象中调用对象中的方法结合表达式实现标签的定位和内容的捕获环境安装如何实例化一个对象导包将本地的文档中的数据加载到该对象中解析本地本件第二个参数最好加上不然可能报错可以将互联网上获取的源码数据加载到该对象中表达式标签定位表达式只能用层级定位定位标签标签的定位最前面的表示从根节点开始一个标签返回一个对象多层级定位一个表示一个多层级也可以表示从任意位置开始定位精准定位精准定位为的选择器写法索引定位索引定位返回第几个元素且索引从开始取直系文本取文本返回的是一个列表取得是直系内容取非直系文本获取标签中非直系的文本内容取属性值取属性值以上所有方法返回的都是列表实战之爬取二手房获取页面源码数据伪装数据解析表示定位到的标签实战之图片解析下载获取页面源码数据伪装手动给响应数据设置编码通用解决中文乱码的解决方案下载完成实战之全国城市名称爬取获取页面源码数据伪装也可以一次获取全部解析热门城市名称解析全部城市名称实战之图片爬取站站站下载完成网站验证码反爬机制验证码识别验证码图片中的数据用于模拟登录操作识别验证码的操作人工肉眼识别不推荐第三方自动识别推荐超级鹰超级鹰使用模拟登录爬取某些用户的相关信息点击登录后会发起一个请求请求会携带相关信息如果参数正确就能进行模拟登录获取的页面请求返回的对象属性值如果是的话就模拟登录成功操作特性无状态没有得到基于第二次请求的个人页面服务器并不知道该请求是基于登录状态下的请求需要存储状态用来让服务器端记录客户端的相关状态不推荐使用手动通过抓包工具获取值在封装到中自动值哪里来模拟登录请求后由服务器创建会话对象可以进行请求的发送如果请求过程中产生了则该会被自动存储携带在该对象中使用对象进行模拟登录请求的发送就会被存储在中对象对个人页面对应的请求进行发送使用对象对全国餐厅进行爬取请输入城市将字符串转换为类型代理理论反爬封同一访问过多会封代理代理服务器代理的作用可以突破自身访问的限制可以隐藏自身真实相关代理网站快代理西祠代理代理类型只能应用于协议的只能应用于协议的代理的匿名度透明服务器知道该次请求使用了代理也知道请求对应的真实匿名知道使用了代理不知道真实高匿不知道使用了代理更不知道真实实现在请求中的参数中输入代理就行异步爬虫目的在爬虫中使用异步实现实现方式多进程多线程不建议好处可以为相关阻塞单独开启线程或者进程阻塞操作就可以异步执行弊端无法无限制开启多线程或者多进程进程池适当使用好处可以减低系统对进程创建和销毁的一个频率从而很好的减低系统的开销弊端进程池线程或进程的数量是有限的实例正在下载下载成功实例化线程池对象将每一个里面的数据传递给处理单线程异步协程推荐事侏循环相当于一个无限循环我们可以把一些函数注册到这个事件循环上当满足某些条件的时候函数就会被循环执行协程对象我们可以将协程对象注册到事件循环中它会被事件循环调用我们可以使用关键字来定义一个方法这个方法在调用时不会立即被执行而是返回一个协程对象任务它是对协程对象的进一步封装包含了任务的各个状态代表将来执行或还没有执行的任务实际上和没有本质区别定义一个协程用来挂起阻塞方法的执行协程用法正在请求请求成功修饰的函数调用之后返回一个协程对象创建一个事件循环对象将协程对象注册到中然后启动使用基于创建了一个对象使用打印协程函数的返回值绑定回调将回调函数绑定到任务对象中协程实例正在下载在异步协程中如果出现了同步模块相关的代码那么久无法实现异步当在中欲打破阻塞操作时必须进行手动挂起下载完毕多任务列表要封装协程模块模块安装实现异步爬虫使用该模块的正在下载模块基于同步基于异步网络请求请求伪装方式和请求参数处理以及代理和库一样但赋值为字符串不再是字典返回字符串的响应数据返回的是二进制的响应数据返回的就是类型的数据注意在获取响应数据之前一定要使用进行手动挂起下载完毕模块为什么需要使用模块可以更快捷的获取网站中动态加载的数据便捷的实现模拟登录什么是模块基于浏览器自动化的一个模块使用环境安装下载一个浏览器的驱动程序浏览器可以自己查找对应浏览器的驱动下载网站实例化一个对象编写浏览器自动化代码初始用实例化一个浏览器对象传入浏览器的驱动器直接写路径也行也可以不用写让浏览器发起一个指定对应请求获取浏览器当前页面的页面源码数据睡眠秒后关闭浏览器相关基础语法发起请求在对象发送请求时的必须带上超文本协议前缀标签定位系列方法标签交互执行程序就代码前进后退关闭浏览器实战之操纵淘宝标签定位标签交互搜索框输入内容执行一组程序向下滚动点击搜索按钮后退前进处理以及动作链如果定位的标签在中则必须要使用动作链拖动导入动作链包实例化一个动作链对象触发长按且点击操作进行拖动动作立即执行释放动作链实操导入动作链的类如果要定位的标签在标签中则进行操作切换定位的作用域动作链点击长安指定的标签立即执行动作链操作释放动作链模拟登录实战之空间登录无头浏览器和规避检测实现无可视化界面实现规避检测如何实现让规避被检测到的风险无可视化界面无头浏览器框架什么是框架就是一个集成了很多功能并且具有很强通用性的一个项目模板如何学习框架什么是爬虫中封装好的一个明星框架高性能的持久化存储异步的数据下载高性能的数据解析分布式如何安装框架创建工程工程名在子目录中创建一个爬虫文件必须进入工程文件爬虫文件名初始初始必须要写执行工程爬虫文件名显示指定类型的日志文件在配置文件中加上查看指定错误的日志文件才显示设置不遵守爬虫协议在配置文件中默认为遵守爬虫协议更改为爬虫文件的名称就是爬虫源文件的一个唯一标识允许的域名用来限定列表中那些可以进行请求的发送一般不使用起始的列表该列表存放的会被自动进行请求的发送用作于数据解析参数就是请求成功后对应的响应对象数据解析解析作者的名称段子内容返回的是列表但是列表元素一定是类型的对象可以将对象中参数存储的字符串提取出来提取第零个元素列表调用之后则表示将列表中每一个对象中的对应的字符串提取出来持久化存储基于终端指令要求只可以将方法的返回值存储到本地的文本文件中指令爬虫文件名文件路径这种持久化存储方式只能存储文件格式优点简介高效便捷缺点局限性比较强数据只能存储到指定后缀文件中基于管道编码流程数据解析在类中定义相关属性文件中的类中用定义属性将解析的数据封装存储到类型的对象将类型的对象交给管道进行持久化存储的操作文件在管道类的方法中要将其接受到的对象中存储的数据进行持久化存储操作在配置文件中开启管道注释解除数值是优先级数值越小优先级越高好处通用性强管道文件中一个管道类对应将一组数据存储到一个平台或载体中在管道文件中加入一个管道类在设置文件中也要相应加入管道类设置在管道文件的方法中最好带上返回值优先级别高的管道类接收数据后会把返回值传递给下一个管道类爬虫文件解析作者的名称段子内容返回的是列表但是列表元素一定是类型的对象可以将对象中参数存储的字符串提取出来提取第零个元素列表调用之后则表示将列表中每一个对象中的对应的字符串提取出来创建类必须使用中括号形式访问类中定义的属性表示将类提交给管道文件文件重写父类的一个方法该方法只在开始爬虫的时候被调用一次开始爬虫专门用来处理类型对象该方法可以接受爬虫文件提交过来的对象该方法每接收到一个就会被调用一次会把返回值交给下一个即将执行的管道类只在爬虫结束的时候调用一次结束爬虫全站数据爬取对一个网站所有的页码对应的页面数据进行爬取实现方式将所有页码添加到列表比较呆板自动手动进行请求发送推荐第一页数据生成一个通用爬虫模块第二页开始手动爬取解析作者的名称段子内容进行手动爬取并在成功获取响应对象时调用方法进行数据解析主要是用来作数据解析五大核心组件调度器包含过滤器队列使用过滤器对进行去重用来接受引擎过来的请求例如队列中并在引擎再次请求的时候返回可以想象成一个抓取网页的网址的优先队列由它来决定下一个要抓取的网址是什么同时取出重复网址管道负责处理爬虫从网页中抽取的实体主要的功能是持久化实体验证实体的有效性清楚不需要的信息当页面被爬虫解析后将被发送到项目管道并进过几个特定的次序处理数据引擎用来处理整个系统的数据流处理触发事物框架核心下载器用于下载网页内容并将网页内容返回给蜘蛛下载器是建立在这个高效的异步模型上的产生进行数据解析爬虫是主要干活的用于从特定的网页中提取自己需要的信息即所谓的实体用户也可以从中提取出链接让继续抓取下一个页面请求传参使用场景如果要爬取解析的数据不在同一张页面中深度爬取传递参数传递的是对象在函数中使用参数进行参数传递该例子为爬取猎聘网站的工作信息且爬取详情页的工作具体描述回调函数接收参数请求传参在请求方法中使用参数可以将字典传递给请求对应的回调函数图片爬取值类基于爬取字符串类型的数据和爬取图片类型的数据区别字符串只需要基于今夕解析且提交管道进行持久化存储图片解析除图片的属性值单独的对图片地址发起请求获取图片二进制类型的数据基于类只需要将的的属性值进行解析提交到管道管道就会对图片的进行请求发送获取图片的二进制类型的数据还会进行持久化存储需求爬取二次元网站的图片使用流程数据解析图片的地址将存储图片地址的提交到制定的管道类在管道文件中自定一个基于的一个管道类并重写三个方法根据图片地址进行图片数据的请求指定图片的存储路径把返回给下一个即将被执行的管道类在配置文件中配置图片下载路径爬虫文件该网站使用了图片懒加载所以除了还要获取属性管道文件可以根据图片地址进行图片数据的请求指定图片进行持久化存储的路径中间件主要有爬虫中间件与下载中间件重点下载中间件处于下载器和引擎之间作用可以批量拦截整个工程中发起的所有的的请求和响应拦截请求伪装代理拦截响应篡改响应数据响应对象使用中间件记得在设置文件中开启对应的设置拦截请求需求爬取百度搜索页爬虫文件文件封装一个池代理池拦截正常请求伪装代理设置为了验证一般卸载拦截所有的响应拦截发生异常的请求代理将修正之后的请求对象进行重新的请求发送拦截响应对象需求爬取网易新闻中的新闻数据标题和内容通过网易新闻的首页解析出四大板块对应的详情页的每一个板块对应的新闻标题都是动态加载出来的通过解析除每一天新闻详情页的获取详情页的页面源码解析出新闻内容爬虫文件存储各个板块的实例化浏览器对象爬虫依次对每个板块的页面进行请求解析每一个板块页面对应新闻的标题和详情页的新闻详情页的发起请求重写父类方法只在爬虫关闭时调用一次文件拦截响应对象进行篡改挑选出指定的响应对象进行篡改通过指定通过指定的实例化一个新的响应对象替代原来不满足需求的响应对象基于便捷的获取动态加载数据把和进行结合替换旧的类类的子类专门用作全站数据的爬取全站数据爬取方式基于手动请求基于的使用只有创建爬虫文件时有所不同文件名链接提取器根据指定规则正则进行指定链接的提取规则解析器将链接提取器提取到的链接进行指定规则的解析操作需求爬取全部页码的古诗文网的诗词题目和详情页的内容爬虫文件规则解析器链接提取器可以将链接提取器继续作用到链接提取器提取到的链接所对应的页面中使用会产生大量重复链接调度器的过滤器会去重如下两个解析方法中是不可以实现请求传参的无法将两个解析方法解析的数据存储到一个中可以存储到两个注意在表达式中不可以出现标签解析详情页内容管道文件如何判定类型设置文件设置日志等级只输出日志信息设置是否遵守爬虫协议设置开启管道在设置文件中解除注释设置图片存储路径图片会都存再文件夹中会自行创建设置开启下载中间件在设置文件中把这解除注释分布式爬虫概念需要搭建一个分布式的机群让其对一组资源进行分布联合爬取作用提升爬取数据的效率如何实现分布式安装的组件原生的不可以实现分布式爬虫必须要让结合组件一起实现分布式爬虫调度器不可以被分布式机群共享管道不可以被分布式机群共享组件作用可以给原生的框架提供可以被共享的管道和调度器实现流程创建一个工程创建一个基于的爬虫文件修改当前爬虫文件导包并且把爬虫类的父类改成将和进行注释添加属性为可以被共享的调度器队列的名称编写数据解析相关操作修改配置文件指定可以被共享的管道指定调度器增加一个去重容器类的配置作用使用的集合来存储请求的指纹数据从而实现请求去重的持久化存储使用组件自己的调度器配置调度器是否要持久化也就是当爬虫结束了要不要清空中请求队列和去重指纹的指定服务器指定数据库相关操作配置配置的配置文件把默认绑定注释掉否则只能自己电脑能访问关闭保护模式否则其他电脑访问只能读数据不能写数据结合配置文件开启服务配置文件启动客户端执行工程爬虫源文件名称向调度器的队列中放入起始调度器的队列在的客户端中队列名称起始爬虫文件可以被共享的调度器队列的名称规则解析器链接提取器可以将链接提取器继续作用到链接提取器提取到的链接所对应的页面中注意在表达式中不可以出现标签其实代码编写步骤和框架差不多很多都是配置上的步骤和的知识关于开发者工具与页面源代码不同有的网站反爬会把某些数据动态加载这时候我们爬不到自己想要的页面这时候我们可以读取页面源代码获取我们想要的数据可以通过抓包工具判断是否为动态加载的数据再决定是否使用模块获取动态数据也可以关于一次获取多个数据语句中可以用符号分隔以获取多个语句的属性关于中文乱码解决方案多页面爬取请求在参数中有页数参数通过修改可以改变页面请求参数中有页面参数通过修改可以改变页面',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-17 10:24:45',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="AYO" type="application/atom+xml">
</head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><div class="back-home-button"><i class="anzhiyufont anzhiyu-icon-grip-vertical"></i><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://git.yuki.love/" title="AYO网站"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="AYO网站"/><span class="back-menu-item-text">AYO网站</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://juejin.cn/user/3708003152300183" title="AYO博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="AYO博客"/><span class="back-menu-item-text">AYO博客</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://github.com/AYO-Al" title="GitHub"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="GitHub"/><span class="back-menu-item-text">GitHub</span></a></div></div></div></div><a id="site-name" href="/" accesskey="h"><div class="title">AYO</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a><div id="he-plugin-simple"></div><script>var WIDGET = {
  "CONFIG": {
    "modules": "0124",
    "background": "2",
    "tmpColor": "FFFFFF",
    "tmpSize": "16",
    "cityColor": "FFFFFF",
    "citySize": "16",
    "aqiColor": "E8D87B",
    "aqiSize": "16",
    "weatherIconSize": "24",
    "alertIconSize": "18",
    "padding": "10px 10px 10px 10px",
    "shadow": "0",
    "language": "auto",
    "borderRadius": "20",
    "fixed": "true",
    "vertical": "top",
    "horizontal": "left",
    "left": "20",
    "top": "7.1",
    "key": "df245676fb434a0691ead1c63341cd94"
  }
}
</script><link rel="stylesheet" href="https://widget.qweather.net/simple/static/css/he-simple.css?v=1.4.0"/><script src="https://widget.qweather.net/simple/static/js/he-simple.js?v=1.4.0"></script></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button only-home" id="travellings_button" title="随机前往一个开往项目网站"><a class="site-page" onclick="anzhiyu.totraveling()" title="随机前往一个开往项目网站" href="javascript:void(0);" rel="external nofollow" data-pjax-state="external"><i class="anzhiyufont anzhiyu-icon-train"></i></a></div><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Ansible/" style="font-size: 1.05rem;">Ansible<sup>1</sup></a><a href="/tags/CDN/" style="font-size: 1.05rem;">CDN<sup>1</sup></a><a href="/tags/DNS/" style="font-size: 1.05rem;">DNS<sup>1</sup></a><a href="/tags/Database/" style="font-size: 1.05rem;">Database<sup>8</sup></a><a href="/tags/Docker/" style="font-size: 1.05rem;">Docker<sup>1</sup></a><a href="/tags/ELK/" style="font-size: 1.05rem;">ELK<sup>1</sup></a><a href="/tags/Flask/" style="font-size: 1.05rem;">Flask<sup>1</sup></a><a href="/tags/Httpd/" style="font-size: 1.05rem;">Httpd<sup>1</sup></a><a href="/tags/JavaScript/" style="font-size: 1.05rem;">JavaScript<sup>1</sup></a><a href="/tags/Keepalived/" style="font-size: 1.05rem;">Keepalived<sup>1</sup></a><a href="/tags/LVS/" style="font-size: 1.05rem;">LVS<sup>1</sup></a><a href="/tags/Linux/" style="font-size: 1.05rem;">Linux<sup>22</sup></a><a href="/tags/MLinux/" style="font-size: 1.05rem;">MLinux<sup>1</sup></a><a href="/tags/MySQL/" style="font-size: 1.05rem;">MySQL<sup>6</sup></a><a href="/tags/Nginx/" style="font-size: 1.05rem;">Nginx<sup>3</sup></a><a href="/tags/Python/" style="font-size: 1.05rem;">Python<sup>8</sup></a><a href="/tags/Rabbit/" style="font-size: 1.05rem;">Rabbit<sup>1</sup></a><a href="/tags/Redis/" style="font-size: 1.05rem;">Redis<sup>2</sup></a><a href="/tags/Shell/" style="font-size: 1.05rem;">Shell<sup>3</sup></a><a href="/tags/Web/" style="font-size: 1.05rem;">Web<sup>2</sup></a><a href="/tags/Zabbix/" style="font-size: 1.05rem;">Zabbix<sup>2</sup></a><a href="/tags/%E4%B8%89%E9%AB%98MySQL/" style="font-size: 1.05rem;">三高MySQL<sup>1</sup></a><a href="/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/" style="font-size: 1.05rem;">中间件<sup>1</sup></a><a href="/tags/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/" style="font-size: 1.05rem;">主从复制<sup>1</sup></a><a href="/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/" style="font-size: 1.05rem;">云计算<sup>7</sup></a><a href="/tags/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/" style="font-size: 1.05rem;">反向代理<sup>1</sup></a><a href="/tags/%E5%9B%9B%E5%A4%A7%E4%BB%B6/" style="font-size: 1.05rem;">四大件<sup>3</sup></a><a href="/tags/%E5%BC%80%E5%8F%91/" style="font-size: 1.05rem;">开发<sup>7</sup></a><a href="/tags/%E6%96%87%E4%BB%B6%E5%85%B1%E4%BA%AB%E6%9C%8D%E5%8A%A1/" style="font-size: 1.05rem;">文件共享服务<sup>1</sup></a><a href="/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/" style="font-size: 1.05rem;">文件系统<sup>1</sup></a><a href="/tags/%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" style="font-size: 1.05rem;">日志管理<sup>1</sup></a><a href="/tags/%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5/" style="font-size: 1.05rem;">时间同步<sup>1</sup></a><a href="/tags/%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/" style="font-size: 1.05rem;">服务管理<sup>1</sup></a><a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 1.05rem;">爬虫<sup>4</sup></a><a href="/tags/%E7%A3%81%E7%9B%98/" style="font-size: 1.05rem;">磁盘<sup>1</sup></a><a href="/tags/%E7%BD%91%E7%BB%9C/" style="font-size: 1.05rem;">网络<sup>4</sup></a><a href="/tags/%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/" style="font-size: 1.05rem;">网络架构<sup>1</sup></a><a href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/" style="font-size: 1.05rem;">自动化运维<sup>1</sup></a><a href="/tags/%E8%BD%AF%E4%BB%B6%E7%AE%A1%E7%90%86/" style="font-size: 1.05rem;">软件管理<sup>1</sup></a><a href="/tags/%E9%98%B2%E7%81%AB%E5%A2%99/" style="font-size: 1.05rem;">防火墙<sup>1</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/05/"><span class="card-archive-list-date">五月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/04/"><span class="card-archive-list-date">四月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/03/"><span class="card-archive-list-date">三月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">7</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">一月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">十二月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">十一月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">十月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/" itemprop="url">Python</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/Python/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Python</span></a><a class="article-meta__tags" href="/tags/%E5%BC%80%E5%8F%91/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>开发</span></a><a class="article-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>爬虫</span></a></span></div></div><h1 class="post-title" itemprop="name headline">聚焦式爬虫</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2021-09-08T16:00:00.000Z" title="发表于 2021-09-09 00:00:00">2021-09-09</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2024-02-17T02:24:45.071Z" title="更新于 2024-02-17 10:24:45">2024-02-17</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-wordcount"><i class="anzhiyufont anzhiyu-icon-file-word post-meta-icon" title="文章字数"></i><span class="post-meta-label" title="文章字数">字数总计:</span><span class="word-count" title="文章字数">10k</span><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-clock post-meta-icon" title="阅读时长"></i><span class="post-meta-label" title="阅读时长">阅读时长:</span><span>42分钟</span></span><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为长沙"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>长沙</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="https://th.bing.com/th/id/R.4cb70989991c1861426358cb196b6d01?rik=83OmAlSQSFM51Q&amp;riu=http%3A%2F%2Fpic.bizhi360.com%2Fbpic%2F88%2F9588_8.jpg&amp;ehk=Vc5LFstc1coz3liGA9YtX3SOl%2BcMJTlRfHNhk%2F4%2FA0w%3D&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;_r_=a160cf65-bb3d-5a5b-fd48-403e56c22880"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://www.yuki.love/2021/09/09/MarkDown/python/%E7%88%AC%E8%99%AB/%E8%81%9A%E7%84%A6%E5%BC%8F%E7%88%AC%E8%99%AB/"><header><a class="post-meta-categories" href="/categories/Python/" itemprop="url">Python</a><a href="/tags/Python/" tabindex="-1" itemprop="url">Python</a><a href="/tags/%E5%BC%80%E5%8F%91/" tabindex="-1" itemprop="url">开发</a><a href="/tags/%E7%88%AC%E8%99%AB/" tabindex="-1" itemprop="url">爬虫</a><h1 id="CrawlerTitle" itemprop="name headline">聚焦式爬虫</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">AYO</span><time itemprop="dateCreated datePublished" datetime="2021-09-08T16:00:00.000Z" title="发表于 2021-09-09 00:00:00">2021-09-09</time><time itemprop="dateCreated datePublished" datetime="2024-02-17T02:24:45.071Z" title="更新于 2024-02-17 10:24:45">2024-02-17</time></header><h2 id="聚焦爬虫"><a href="#聚焦爬虫" class="headerlink" title="聚焦爬虫"></a>聚焦爬虫</h2><ul>
<li>概述： 爬取页面中指定的页面内容</li>
<li>编码流程<ol>
<li>指定url</li>
<li>发起请求</li>
<li>获取响应数据</li>
<li>数据解析</li>
<li>持久化存储</li>
</ol>
</li>
</ul>
<h2 id="1-数据解析"><a href="#1-数据解析" class="headerlink" title="1. 数据解析"></a>1. 数据解析</h2><h3 id="1-1-数据解析分类"><a href="#1-1-数据解析分类" class="headerlink" title="1.1 数据解析分类"></a>1.1 数据解析分类</h3><ul>
<li>正则</li>
<li>bs4</li>
<li>xpath(主要)</li>
</ul>
<h3 id="1-2-数据解析原理"><a href="#1-2-数据解析原理" class="headerlink" title="1.2 数据解析原理"></a>1.2 数据解析原理</h3><ul>
<li>概述：<ul>
<li>解析的局部文本内容都会在标签之间或者标签对应的属性中进行存储</li>
</ul>
</li>
<li>方法：<ol>
<li>进行指定标签的定位</li>
<li>标签或者标签对应的属性中存储的数据值进行提取(解析)</li>
</ol>
</li>
</ul>
<h2 id="2-正则解析"><a href="#2-正则解析" class="headerlink" title="2. 正则解析"></a>2. 正则解析</h2><h3 id="2-1-正则表达式"><a href="#2-1-正则表达式" class="headerlink" title="2.1 正则表达式"></a>2.1 正则表达式</h3><ul>
<li>正则：用来匹配字符串的一门表达式语言</li>
</ul>
<h3 id="2-2-常用正则表达式"><a href="#2-2-常用正则表达式" class="headerlink" title="2.2 常用正则表达式"></a>2.2 常用正则表达式</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://note.youdao.com/yws/public/resource/92933d878935240680c00793fc3e404c/xmlnote/WEBRESOURCE47a9a1103d33ebf7b86d1ab7aae51338/31" alt="正则"></p>
<h3 id="2-2-实战之暴走漫画"><a href="#2-2-实战之暴走漫画" class="headerlink" title="2.2 实战之暴走漫画"></a>2.2 实战之暴走漫画</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line">import requests</span><br><span class="line">import os</span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    #创建一个文件夹，保存所有图片</span><br><span class="line">    if not os.path.exists(&#x27;./暴走&#x27;):</span><br><span class="line">        os.mkdir(&#x27;./暴走&#x27;)</span><br><span class="line">    url=&#x27;http://admin2.baozoumanhua.com/&#x27;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36 Edg/97.0.1072.62&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    #获取全部数据</span><br><span class="line">    page_text=requests.get(url=url,headers=headers).text</span><br><span class="line">    #使用正则对图片进行解析</span><br><span class="line">    ex = &#x27;&lt;img.*? src=&quot;(.*?)&quot; .*?/&gt;&#x27;</span><br><span class="line">    img_src_list=re.findall(ex,page_text,re.S)</span><br><span class="line">    for i in img_src_list:</span><br><span class="line">    # content方法获取二进制数据类型</span><br><span class="line">        img_data=requests.get(url=i,headers=headers).content</span><br><span class="line">        #生成图片名称</span><br><span class="line">        img_name=i.split(&#x27;/&#x27;)[-1]</span><br><span class="line">        imgPath=&#x27;./暴走/&#x27;+img_name</span><br><span class="line">        with open(imgPath,&#x27;wb&#x27;)as fp:</span><br><span class="line">            fp.write(img_data)</span><br><span class="line">            print(img_name+&#x27;下载成功&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="bs4解析"><a href="#bs4解析" class="headerlink" title="bs4解析"></a>bs4解析</h2><ul>
<li><p>只能在python用</p>
</li>
<li><p>bs4数据解析原理</p>
<ul>
<li>实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中</li>
<li>通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取</li>
</ul>
</li>
<li><p>环境安装：</p>
<ul>
<li>pip install bs4</li>
<li>pip install lxml</li>
</ul>
</li>
<li><p>如何实例化BeautifulSoup对象</p>
<ul>
<li>from bs4 import BeautifulSoup</li>
<li>对象实例化<ol>
<li>将本地的HTML文档中的数据加载到该对象中<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">  #将本地的html文档中的数据加载到该对象中</span><br><span class="line">  fp=open(&#x27;D:\\html\\登录.html&#x27;,&#x27;r&#x27;,encoding=&#x27;utf-8&#x27;)</span><br><span class="line">  #第二个参数为BeautifulSoup对象使用lxml解析器进行解析</span><br><span class="line">  soup=BeautifulSoup(fp,&#x27;lxml&#x27;)</span><br><span class="line">  print(soup)</span><br></pre></td></tr></table></figure></li>
<li>将互联网上获取的页面源码加载到该对象中<ul>
<li>与第一种差不多，只要先把爬取到的全部数据存到本地文档在解析</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>提供的用于数据解析解析的属性和方法：</p>
<ul>
<li>soup.tagName：返回html中第一次出现的tagName标签</li>
<li>soup.find(‘tagName’):返回第一次出现的tagName对应的标签<ul>
<li>属性定位：<ul>
<li>soup.find(‘tagName’，class_&#x2F;id&#x2F;attr&#x3D;’’)</li>
</ul>
</li>
</ul>
</li>
<li>soup.find_all(‘tagName’):返回所有出现的tagName对应的标签(列表)</li>
<li>soup.select(“某种选择器”)：返回一个列表</li>
<li>soup.select(“某种选择器&gt;某种选择器 某种选择器”)：&gt;表示一个层级，空格表示多个层级</li>
<li>获取标签中的文本数据<ul>
<li>soup.a.text&#x2F;string&#x2F;get_text()<ul>
<li>soup.a.text&#x2F;get_text():获取一个标签里的所有文本内容</li>
<li>soup.a.string：获取一个标签直系的文本内容</li>
</ul>
</li>
<li>获取标签中的属性值<ul>
<li>soup.a[‘href’]</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="xpath解析"><a href="#xpath解析" class="headerlink" title="xpath解析"></a>xpath解析</h2><ul>
<li>最常用切最便捷高效的一种方式</li>
<li>xpath解析原理：<ol>
<li>实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中</li>
<li>调用etree对象中的xpath方法结合xpath表达式实现标签的定位和内容的捕获</li>
</ol>
</li>
<li>环境安装<ul>
<li>pip install lxml</li>
</ul>
</li>
<li>如何实例化一个etree对象<ol>
<li>导包：from lxml import etree</li>
<li>将本地的HTML文档中的数据加载到该对象中：<ul>
<li>etree.parse(filePath,etree.HTMLParser())</li>
<li>解析本地本件第二个参数最好加上，不然可能报错</li>
</ul>
</li>
<li>可以将互联网上获取的源码数据加载到该对象中<ul>
<li>etree.HTML(‘page_text’)</li>
</ul>
</li>
</ol>
</li>
<li>xpath(‘xpath表达式’)<ul>
<li>标签定位：<ul>
<li>xpath表达式只能用层级定位定位标签<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 标签的定位</span><br><span class="line">#最前面的/表示从根节点开始</span><br><span class="line"># 一个标签返回一个element对象</span><br><span class="line">r=tree.xpath(&#x27;/html/head/title&#x27;)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>多层级定位<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#一个//表示一个多层级，也可以表示从任意位置开始定位</span><br><span class="line">r=tree.xpath(&#x27;/html//title&#x27;)</span><br></pre></td></tr></table></figure></li>
<li>精准定位<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 精准定位class为song的divs</span><br><span class="line"># 选择器写法[@attrName=&#x27;attrValue&#x27;]</span><br><span class="line">r = tree.xpath(&#x27;//div[@class=&quot;ong&quot;]&#x27;)</span><br></pre></td></tr></table></figure></li>
<li>索引定位<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 索引定位,返回第几个元素，且索引从1开始</span><br><span class="line">r = tree.xpath(&#x27;//div[@class=&quot;song&quot;]/p[3]&#x27;)</span><br></pre></td></tr></table></figure></li>
<li>取直系文本<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 取文本,text()返回的是一个列表,取得是直系内容</span><br><span class="line"># /text()</span><br><span class="line">  r = tree.xpath(&#x27;//div[@class=&quot;song&quot;]//li[5]/a/text()&#x27;)</span><br></pre></td></tr></table></figure></li>
<li>取非直系文本<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#获取标签中非直系的文本内容</span><br><span class="line">r = tree.xpath(&#x27;//li[7]//text()&#x27;)</span><br></pre></td></tr></table></figure></li>
<li>取属性值<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#取属性值</span><br><span class="line">r = tree.xpath(&#x27;//div[@class=&quot;song&quot;]/img/@src&#x27;)[0]</span><br></pre></td></tr></table></figure></li>
<li>以上所有xpath方法返回的都是列表</li>
</ul>
</li>
</ul>
<h3 id="xpath实战之爬取58二手房"><a href="#xpath实战之爬取58二手房" class="headerlink" title="xpath实战之爬取58二手房"></a>xpath实战之爬取58二手房</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">from lxml import etree</span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    # 获取页面源码数据</span><br><span class="line">    url=&#x27;https://bj.58.com/ershoufang/&#x27;</span><br><span class="line">    # UA伪装</span><br><span class="line">    head=&#123;</span><br><span class="line">        &#x27;User-Agent&#x27;:&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.69&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    page_text=requests.get(url=url,headers=head).text</span><br><span class="line">    #数据解析</span><br><span class="line">    tree =etree.HTML(page_text)</span><br><span class="line">    list=tree.xpath(&#x27;//div[@class=&quot;property-content-detail&quot;]&#x27;)</span><br><span class="line">    fp=open(&#x27;58.txt&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;)</span><br><span class="line">    for h3 in list:</span><br><span class="line">        #./表示定位到的div标签</span><br><span class="line">        title=h3.xpath(&#x27;.//text()&#x27;)[0]</span><br><span class="line">        print(title)</span><br><span class="line">        fp.write(title+&#x27;\n&#x27;)</span><br><span class="line">    fp.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="xpath实战之4k图片解析下载"><a href="#xpath实战之4k图片解析下载" class="headerlink" title="xpath实战之4k图片解析下载"></a>xpath实战之4k图片解析下载</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">from lxml import etree</span><br><span class="line">import os</span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    # 获取页面源码数据</span><br><span class="line">    url=&#x27;https://pic.netbian.com/4kmeinv/&#x27;</span><br><span class="line">    # UA伪装</span><br><span class="line">    head = &#123;</span><br><span class="line">        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.69&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    response = requests.get(url=url, headers=head)</span><br><span class="line">    #手动给响应数据设置编码</span><br><span class="line">    # response.encoding=&#x27;gbk&#x27;</span><br><span class="line">    page_text=response.text</span><br><span class="line">    tree=etree.HTML(page_text)</span><br><span class="line">    li_list=tree.xpath(&#x27;//div[@class=&quot;slist&quot;]/ul/li&#x27;)</span><br><span class="line">    if not os.path.exists(&#x27;./picLibs&#x27;):</span><br><span class="line">        os.mkdir(&#x27;./picLibs&#x27;)</span><br><span class="line">    for li in li_list:</span><br><span class="line">        img_src=&#x27;https://pic.netbian.com&#x27;+li.xpath(&#x27;./a/img/@src&#x27;)[0]</span><br><span class="line">        img_name=li.xpath(&#x27;./a/img/@alt&#x27;)[0]+&#x27;.jpg&#x27;</span><br><span class="line">        # 通用解决中文乱码的解决方案</span><br><span class="line">        img_name=img_name.encode(&#x27;iso-8859-1&#x27;).decode(&#x27;gbk&#x27;)</span><br><span class="line">        # print(img_name,img_src)</span><br><span class="line">        img_data=requests.get(url=img_src,headers=head).content</span><br><span class="line">        img_path=&#x27;picLibs/&#x27;+img_name</span><br><span class="line">        with open(img_path,&#x27;wb&#x27;)as fp:</span><br><span class="line">            fp.write(img_data)</span><br><span class="line">            print(img_name+&#x27;下载完成！！&#x27;)</span><br></pre></td></tr></table></figure>
<h3 id="xpath实战之全国城市名称爬取"><a href="#xpath实战之全国城市名称爬取" class="headerlink" title="xpath实战之全国城市名称爬取"></a>xpath实战之全国城市名称爬取</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">from lxml import etree</span><br><span class="line">import os</span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    # 获取页面源码数据</span><br><span class="line">    url=&#x27;https://www.aqistudy.cn/historydata/&#x27;</span><br><span class="line">    # UA伪装</span><br><span class="line">    head = &#123;</span><br><span class="line">        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.69&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    page_text = requests.get(url=url, headers=head).text</span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    host_li_list=tree.xpath(&#x27;//div[@class=&quot;bottom&quot;]/ul/li&#x27;)</span><br><span class="line">    all_city_names=[]</span><br><span class="line">    # 也可以一次获取全部</span><br><span class="line">    # tree.xpath(&#x27;//div[@class=&quot;bottom&quot;]/ul/li/a | //div[@class=&quot;bottom&quot;]/ul/div[2]/li/a&#x27;)</span><br><span class="line">    #解析热门城市名称</span><br><span class="line">    for li in host_li_list:</span><br><span class="line">        host_city_name=li.xpath(&#x27;./a/text()&#x27;)[0]</span><br><span class="line">        all_city_names.append(host_city_name)</span><br><span class="line">    city_names_list=tree.xpath(&#x27;//div[@class=&quot;bottom&quot;]/ul/div[2]/li&#x27;)</span><br><span class="line">    #解析全部城市名称</span><br><span class="line">    for li in city_names_list:</span><br><span class="line">        city_name=li.xpath(&#x27;./a/text()&#x27;)[0]</span><br><span class="line">        all_city_names.append(city_name)</span><br><span class="line">    print(all_city_names,len(all_city_names))</span><br></pre></td></tr></table></figure>
<h3 id="xpath实战之图片爬取"><a href="#xpath实战之图片爬取" class="headerlink" title="xpath实战之图片爬取"></a>xpath实战之图片爬取</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">import os</span><br><span class="line">from lxml import etree</span><br><span class="line">if __name__==&#x27;__main__&#x27;:</span><br><span class="line">    lxm=0</span><br><span class="line">    url = &#x27;https://www.vilipix.com/ranking&#x27;</span><br><span class="line">    head = &#123;</span><br><span class="line">        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36 Edg/97.0.1072.69&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    page_text=requests.get(url=url,headers=head).text</span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    img_list=tree.xpath(&#x27;//div[@class=&quot;title&quot;]/a&#x27;)</span><br><span class="line">    if not os.path.exists(&#x27;./p站&#x27;):</span><br><span class="line">        os.mkdir(&#x27;./p站&#x27;)</span><br><span class="line">    for i in img_list:</span><br><span class="line">        img_url=&#x27;https://www.vilipix.com&#x27;+i.xpath(&#x27;./@href&#x27;)[0]</span><br><span class="line">        img_data = requests.get(url=img_url, headers=head).text</span><br><span class="line">        ptree=etree.HTML(img_data)</span><br><span class="line">        p_list=ptree.xpath(&#x27;//a[@href=&quot;javascript: void(0)&quot;]/img&#x27;)</span><br><span class="line">        for img in p_list:</span><br><span class="line">            lxm+=1</span><br><span class="line">            img_p=img.xpath(&#x27;./@src&#x27;)[0]</span><br><span class="line">            pp=requests.get(url=img_p,headers=head).content</span><br><span class="line">            img_name=img.xpath(&#x27;./@alt&#x27;)[0]+str(lxm)+&#x27;.jpg&#x27;</span><br><span class="line">            img_path=&#x27;p站/&#x27;+img_name</span><br><span class="line">            img_path=img_path.replace(&quot;?&quot;,&quot;L&quot;)</span><br><span class="line">            with open(img_path,&#x27;wb&#x27;) as fp:</span><br><span class="line">                fp.write(pp)</span><br><span class="line">                print(img_name+&#x27;下载完成！！&#x27;)</span><br><span class="line">    print(&quot;over!!!!!&quot;)</span><br></pre></td></tr></table></figure>

<h2 id="网站验证码"><a href="#网站验证码" class="headerlink" title="网站验证码"></a>网站验证码</h2><ul>
<li>反爬机制：验证码。识别验证码图片中的数据，用于模拟登录操作</li>
<li>识别验证码的操作：<ul>
<li>人工肉眼识别（不推荐）</li>
<li>第三方自动识别（推荐）<ul>
<li>超级鹰：<a target="_blank" rel="noopener" href="http://www.chaojiying.com/demo.html">http://www.chaojiying.com/demo.html</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="超级鹰使用"><a href="#超级鹰使用" class="headerlink" title="超级鹰使用"></a>超级鹰使用</h3><h2 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h2><ul>
<li>爬取某些用户的相关信息</li>
<li>点击登录后会发起一个post请求</li>
<li>post请求会携带相关信息，如果参数正确就能进行模拟登录</li>
<li>获取的页面请求返回的对象status_code属性值如果是200的话就模拟登录成功</li>
</ul>
<h2 id="cookie操作"><a href="#cookie操作" class="headerlink" title="cookie操作"></a>cookie操作</h2><ul>
<li><p>http&#x2F;https特性：无状态</p>
</li>
<li><p>没有得到基于第二次请求的个人页面，服务器并不知道该请求是基于登录状态下的请求</p>
</li>
<li><p>需要cookie存储状态</p>
</li>
<li><p>cookie：用来让服务器端记录客户端的相关状态</p>
</li>
<li><p>不推荐使用手动cookie：通过抓包工具获取cookie值，在封装到headers中</p>
</li>
<li><p>自动cookie：</p>
<ul>
<li><p>cookie值哪里来？</p>
<ul>
<li>模拟登录post请求后，由服务器创建</li>
</ul>
</li>
<li><p>session会话对象：</p>
<ul>
<li>可以进行请求的发送</li>
<li>如果请求过程中产生了cookie，则该cookie会被自动存储&#x2F;携带在该对象中</li>
<li>使用session对象进行模拟登录post请求的发送（cookie就会被存储在session中）</li>
<li>session&#x3D;requests.Session()</li>
<li>session对象对个人页面对应的get请求进行发送</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用session对象对全国kfc餐厅进行爬取</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    url=<span class="string">&#x27;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?&#x27;</span></span><br><span class="line">    key=<span class="built_in">input</span>(<span class="string">&#x27;请输入城市：&#x27;</span>)</span><br><span class="line">    par=&#123;</span><br><span class="line">        <span class="string">&#x27;op&#x27;</span>:<span class="string">&#x27;keyword&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;cname&#x27;</span>:<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pid&#x27;</span>:<span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;keyword&#x27;</span>: key,</span><br><span class="line">        <span class="string">&#x27;pageIndex&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pageSize&#x27;</span>: <span class="string">&#x27;100&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36 Edg/97.0.1072.62&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    ss=requests.session()</span><br><span class="line">    response = ss.get(url=url,params=par,headers=headers)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(response))</span><br><span class="line">    page_text=response.text</span><br><span class="line">    <span class="comment"># 将字符串转换为json类型</span></span><br><span class="line">    page_json=json.loads(page_text)</span><br><span class="line">    fp=<span class="built_in">open</span>(<span class="string">&#x27;./kfc.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    json.dump(page_json,fp=fp,ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;over!!!&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="代理理论"><a href="#代理理论" class="headerlink" title="代理理论"></a>代理理论</h2><ul>
<li>反爬封ip：同一ip访问过多会封ip</li>
<li>代理：代理服务器</li>
<li>代理的作用：<ul>
<li>可以突破自身ip访问的限制</li>
<li>可以隐藏自身真实ip</li>
</ul>
</li>
<li>相关代理网站：<ul>
<li>快代理</li>
<li>西祠代理</li>
<li><a target="_blank" rel="noopener" href="http://www.goubanjia.com/">www.goubanjia.com</a></li>
</ul>
</li>
<li>代理ip类型<ul>
<li>http：只能应用于http协议的url</li>
<li>https：只能应用于https协议的url</li>
</ul>
</li>
<li>代理ip的匿名度：<ul>
<li>透明：服务器知道该次请求使用了代理，也知道请求对应的真实ip</li>
<li>匿名：知道使用了代理，不知道真实ip</li>
<li>高匿：不知道使用了代理，更不知道真实ip</li>
</ul>
</li>
<li>实现：<ul>
<li>在请求中的proxies参数中输入代理ip就行</li>
<li><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://note.youdao.com/yws/public/resource/92933d878935240680c00793fc3e404c/xmlnote/WEBRESOURCE33b5a9f1dcbabfc93f46a4762deb1cb6/33" alt="代理ip"></li>
</ul>
</li>
</ul>
<h2 id="异步爬虫"><a href="#异步爬虫" class="headerlink" title="异步爬虫"></a>异步爬虫</h2><ul>
<li>目的：在爬虫中使用异步实现</li>
<li>实现方式：<ol>
<li>多进程，多线程（不建议）<ul>
<li>好处：可以为相关阻塞单独开启线程或者进程，阻塞操作就可以异步执行</li>
<li>弊端：无法无限制开启多线程或者多进程</li>
</ul>
</li>
<li>进程池：（适当使用）<ul>
<li>好处：可以减低系统对进程创建和销毁的一个频率，从而很好的减低系统的开销</li>
<li>弊端：进程池线程或进程的数量是有限的 </li>
<li>实例： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import time</span><br><span class="line">from multiprocessing.dummy import Pool</span><br><span class="line">start_time=time.time()</span><br><span class="line">def get_page(str):</span><br><span class="line">    print(&#x27;正在下载：&#x27;,str)</span><br><span class="line">    time.sleep(2)</span><br><span class="line">    print(&#x27;下载成功&#x27;,str)</span><br><span class="line">name_list=[&#x27;aa&#x27;,&#x27;bb&#x27;,&#x27;cc&#x27;]</span><br><span class="line">#实例化线程池对象</span><br><span class="line">pool=Pool(4)</span><br><span class="line">#将每一个name_list里面的数据传递给get_page处理</span><br><span class="line">pool.map(get_page,name_list)</span><br><span class="line">end_time=time.time()</span><br><span class="line">print(&quot;%d second&quot;%(end_time-start_time))</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>单线程+异步协程（推荐）</li>
</ol>
<ul>
<li>event_loop:事侏循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上,当满足某些条件的时候,函数就会被循环执行。</li>
<li>coroutine:协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用。我们可以使用async 关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回一个协程对象。</li>
<li>task:任务，它是对协程对象的进一步封装，包含了任务的各个状态。</li>
<li>future:代表将来执行或还没有执行的任务，实际上和task没有本质区别.</li>
<li>async定义一个协程.</li>
<li>await用来挂起阻塞方法的执行。</li>
<li>协程用法：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import asyncio</span><br><span class="line">async def request(url):</span><br><span class="line">    print(&#x27;正在请求：&#x27;,url)</span><br><span class="line">    print(&#x27;请求成功：&#x27;,url)</span><br><span class="line">    return url</span><br><span class="line">#asunc修饰的函数，调用之后返回一个协程对象</span><br><span class="line">c=request(&#x27;www.baidu.com&#x27;)</span><br><span class="line">#创建一个事件循环对象</span><br><span class="line"># loop=asyncio.get_event_loop()</span><br><span class="line"># #将协程对象注册到loop中，然后启动loop</span><br><span class="line"># loop.run_until_complete(c)</span><br><span class="line">#task使用</span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">#基于loop创建了一个task对象</span><br><span class="line"># task=loop.create_task(c)</span><br><span class="line"># print(task)</span><br><span class="line"># loop.run_until_complete(task)</span><br><span class="line">#future使用</span><br><span class="line"># loop=asyncio.get_event_loop()</span><br><span class="line"># task=asyncio.ensure_future(c)</span><br><span class="line"># print(task)</span><br><span class="line"># loop.run_until_complete(task)</span><br><span class="line"># print(task)</span><br><span class="line">def callback_func(task):</span><br><span class="line">    #打印协程函数的返回值</span><br><span class="line">    print(task.result())</span><br><span class="line">#绑定回调</span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">task=asyncio.ensure_future(c)</span><br><span class="line"># 将回调函数绑定到任务对象中</span><br><span class="line">task.add_done_callback(callback_func)</span><br><span class="line">loop.run_until_complete(task)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>协程实例：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import asyncio</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">async  def request(url):</span><br><span class="line">    print(&#x27;正在下载&#x27;,url)</span><br><span class="line">    #在异步协程中如果出现了同步模块相关的代码，那么久无法实现异步</span><br><span class="line">    # time.sleep(2)</span><br><span class="line">    #当在asyncio中欲打破阻塞操作时必须进行手动挂起</span><br><span class="line">    await asyncio.sleep(2)</span><br><span class="line">    print(&#x27;下载完毕&#x27;,url)</span><br><span class="line">start=time.time()</span><br><span class="line">urls=[</span><br><span class="line">    &#x27;www.baidu.com&#x27;,</span><br><span class="line">    &#x27;www.sogou.com&#x27;,</span><br><span class="line">    &#x27;www.goubanjia.com&#x27;</span><br><span class="line">]</span><br><span class="line">stasks=[]</span><br><span class="line">for url in urls:</span><br><span class="line">    c=request(url)</span><br><span class="line">    task=asyncio.ensure_future(c)</span><br><span class="line">    stasks.append(task)</span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">#多任务列表要封装wait()</span><br><span class="line">loop.run_until_complete(asyncio.wait(stasks))</span><br><span class="line">print(time.time()-start)</span><br></pre></td></tr></table></figure></li>
<li>协程aiohttp模块<ul>
<li>模块安装：pip install aiohttp</li>
<li>实现异步爬虫<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">import asyncio</span><br><span class="line">import time</span><br><span class="line">import aiohttp</span><br><span class="line"># 使用该模块的ClientSession</span><br><span class="line">start=time.time()</span><br><span class="line">urls=[</span><br><span class="line">    &#x27;http://baidu.com&#x27;,</span><br><span class="line">    &#x27;http://sogou.com&#x27;,</span><br><span class="line">    &#x27;http://baidu.com&#x27;,</span><br><span class="line">]</span><br><span class="line">async def get_page(url):</span><br><span class="line">    print(&#x27;正在下载&#x27;,url)</span><br><span class="line">    # requests模块基于同步</span><br><span class="line">    # aiohttpj基于异步网络请求</span><br><span class="line">    # response=requests.get(url=url)</span><br><span class="line">    async with aiohttp.ClientSession() as session:</span><br><span class="line">        # async  with await session.post(url) as response:post请求</span><br><span class="line">        # UA伪装方式和请求参数处理以及代理ip和requests库一样，但ip赋值为字符串，不再是字典</span><br><span class="line">      async  with await session.get(url) as response:</span><br><span class="line">          # text()返回字符串的响应数据</span><br><span class="line">          # read()返回的是二进制的响应数据</span><br><span class="line">          # json()返回的就是json类型的数据</span><br><span class="line">          # 注意：在获取响应数据之前一定要使用await进行手动挂起</span><br><span class="line">          page_text=await response.text()</span><br><span class="line">    print(&#x27;下载完毕&#x27;, page_text)</span><br><span class="line">tasks=[]</span><br><span class="line">for url in urls:</span><br><span class="line">    c=get_page(url)</span><br><span class="line">    task=asyncio.ensure_future(c)</span><br><span class="line">    tasks.append(task)</span><br><span class="line">loop=asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">print(time.time()-start)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="selenium模块"><a href="#selenium模块" class="headerlink" title="selenium模块"></a>selenium模块</h2><ul>
<li>为什么需要使用selenium模块？<ul>
<li>selenium可以更快捷的获取网站中动态加载的数据</li>
<li>便捷的实现模拟登录</li>
</ul>
</li>
<li>什么是selenium模块？<ul>
<li>基于浏览器自动化的一个模块</li>
</ul>
</li>
<li>selenium使用<ul>
<li>环境安装：pip install selenium</li>
<li>下载一个浏览器的驱动程序<ul>
<li><a target="_blank" rel="noopener" href="https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/%EF%BC%88edge%E6%B5%8F%E8%A7%88%E5%99%A8%EF%BC%89">https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/（edge浏览器）</a></li>
<li>可以自己查找对应浏览器的驱动下载网站</li>
</ul>
</li>
<li>实例化一个selenium对象</li>
<li>编写浏览器自动化代码</li>
<li>初始用<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from lxml import etree</span><br><span class="line">from time import sleep</span><br><span class="line"># 实例化一个浏览器对象(传入浏览器的驱动器)</span><br><span class="line"># 直接写路径也行,也可以不用写</span><br><span class="line"># bro=webdriver.Edge(&#x27;msedgedriver.exe&#x27;)</span><br><span class="line">bro=webdriver.Edge(&#x27;msedgedriver.exe&#x27;)</span><br><span class="line"># 让浏览器发起一个指定url对应请求</span><br><span class="line">bro.get(&#x27;http://scxk.nmpa.gov.cn:81/xk/&#x27;)</span><br><span class="line"># 获取浏览器当前页面的页面源码数据</span><br><span class="line">page_text=bro.page_source</span><br><span class="line">tree=etree.HTML(page_text)</span><br><span class="line">li_list=tree.xpath(&#x27;//ul[@id=&quot;gzlist&quot;]/li&#x27;)</span><br><span class="line">for li in li_list:</span><br><span class="line">    name=li.xpath(&#x27;./dl/@title&#x27;)[0]</span><br><span class="line">    print(name)</span><br><span class="line"># 睡眠5秒后关闭浏览器</span><br><span class="line">sleep(5)</span><br><span class="line">bro.quit()</span><br></pre></td></tr></table></figure></li>
<li>相关基础语法<ul>
<li>发起请求：get()<ul>
<li>在selenium对象发送请求时的url必须带上超文本协议前缀</li>
</ul>
</li>
<li>标签定位：find系列方法</li>
<li>标签交互：send_keys(‘xxx’)</li>
<li>执行js程序：excut_script(‘就js代码’)</li>
<li>前进、后退：forward()、back()</li>
<li>关闭浏览器：quit()</li>
</ul>
</li>
<li>实战之操纵淘宝<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from time import sleep</span><br><span class="line">bro=webdriver.Edge()</span><br><span class="line">bro.get(&#x27;https://www.taobao.com/&#x27;)</span><br><span class="line"># 标签定位</span><br><span class="line">serch_input=bro.find_element_by_id(&#x27;q&#x27;)</span><br><span class="line"># 标签交互，搜索框输入内容</span><br><span class="line">serch_input.send_keys(&#x27;Iphone&#x27;)</span><br><span class="line"># 执行一组js程序</span><br><span class="line"># 向下滚动</span><br><span class="line">bro.execute_script(&#x27;window.scrollTo(0,document.body.scrollHeight)&#x27;)</span><br><span class="line">btn=bro.find_element_by_css_selector(&#x27;.btn-search&#x27;)</span><br><span class="line"># 点击搜索按钮</span><br><span class="line">btn.click()</span><br><span class="line">bro.get(&#x27;https://www.baidu.com&#x27;)</span><br><span class="line"># 后退</span><br><span class="line">sleep(2)</span><br><span class="line">bro.back()</span><br><span class="line"># 前进</span><br><span class="line">sleep(2)</span><br><span class="line">bro.forward()</span><br><span class="line">sleep(5)</span><br><span class="line">bro.quit()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>selenium处理iframe以及动作链<ul>
<li>如果定位的标签在iframe中则必须要使用switch_to.frame(‘id’)</li>
<li>动作链（拖动）：from selenium.webdriver import ActionChains导入动作链包<ul>
<li>实例化一个动作链对象<ul>
<li>action&#x3D;ActionChains(bro)</li>
</ul>
</li>
<li>触发长按且点击操作<ul>
<li>action.click_and_hold(div)</li>
</ul>
</li>
<li>进行拖动<ul>
<li>action.move_by_offset(17,10)</li>
</ul>
</li>
<li>动作立即执行<ul>
<li>perform()</li>
</ul>
</li>
<li>释放动作链<ul>
<li>action.release()</li>
</ul>
</li>
</ul>
</li>
<li>实操：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from time import sleep</span><br><span class="line"># 导入动作链的类</span><br><span class="line">from selenium.webdriver import ActionChains</span><br><span class="line">bro=webdriver.Edge()</span><br><span class="line">bro.get(&#x27;https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&#x27;)</span><br><span class="line"># 如果要定位的标签在iframe标签中，则进行操作</span><br><span class="line">bro.switch_to.frame(&#x27;iframeResult&#x27;)# 切换定位的作用域</span><br><span class="line">div=bro.find_element_by_id(&#x27;draggable&#x27;)</span><br><span class="line"># 动作链</span><br><span class="line">action=ActionChains(bro)</span><br><span class="line"># 点击长安指定的标签</span><br><span class="line">action.click_and_hold(div)</span><br><span class="line">for i in range(5):</span><br><span class="line">    # perform()立即 执行动作链操作</span><br><span class="line">    action.move_by_offset(17,10).perform()</span><br><span class="line">    sleep(0.2)</span><br><span class="line"># 释放动作链</span><br><span class="line">action.release()</span><br><span class="line">bro.quit()</span><br></pre></td></tr></table></figure></li>
<li>模拟登录</li>
<li>实战之QQ空间登录<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from time import sleep</span><br><span class="line">bro=webdriver.Edge()</span><br><span class="line">bro.get(&#x27;https://qzone.qq.com/&#x27;)</span><br><span class="line">bro.switch_to.frame(&#x27;login_frame&#x27;)</span><br><span class="line">a_tag=bro.find_element_by_id(&quot;img_out_2609320892&quot;)</span><br><span class="line">a_tag.click()</span><br></pre></td></tr></table></figure></li>
<li>无头浏览器和规避检测<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from time import sleep</span><br><span class="line"># 实现无可视化界面</span><br><span class="line"># 实现规避检测</span><br><span class="line">from selenium.webdriver.edge.options import Options</span><br><span class="line">edge_options=Options()</span><br><span class="line">edge_options.add_argument(&#x27;--headless&#x27;)</span><br><span class="line">edge_options.add_argument(&#x27;--disable-gpu&#x27;)</span><br><span class="line"># 如何实现让selenium规避被检测到的风险</span><br><span class="line">edge_options.add_experimental_option(&#x27;excludeSwitches&#x27;,[&#x27;enable-automation&#x27;])</span><br><span class="line">bro=webdriver.Edge(options=edge_options)</span><br><span class="line"># 无可视化界面（无头浏览器），phantomJs</span><br><span class="line">bro.get(&#x27;https://baidu.com&#x27;)</span><br><span class="line">print(bro.page_source)</span><br><span class="line">bro.quit()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="scrapy框架"><a href="#scrapy框架" class="headerlink" title="scrapy框架"></a>scrapy框架</h2><ul>
<li>什么是框架？<ul>
<li>就是一个集成了很多功能并且具有很强通用性的一个项目模板</li>
</ul>
</li>
<li>如何学习框架？<ul>
<li><p>什么是scrapy？</p>
</li>
<li><p>爬虫中封装好的一个明星框架</p>
</li>
<li><p>高性能的持久化存储</p>
</li>
<li><p>异步的数据下载</p>
</li>
<li><p>高性能的数据解析</p>
</li>
<li><p>分布式</p>
</li>
</ul>
</li>
<li>如何安装scrapy框架？<ul>
<li>mac or linux：pip install scrapy</li>
<li>windows：pip install scrapy</li>
</ul>
</li>
</ul>
<ol>
<li><p>创建工程</p>
<ul>
<li>scrapy startproject [工程名]</li>
</ul>
</li>
<li><p>在spiders子目录中创建一个爬虫文件</p>
<ul>
<li><p>必须进入工程文件</p>
</li>
<li><p>scrapy genspider [爬虫文件名] [初始url] </p>
</li>
<li><p>初始url必须要写</p>
</li>
</ul>
</li>
<li><p>执行工程</p>
<ul>
<li>scrapy crawl [爬虫文件名]</li>
</ul>
</li>
<li><p>显示指定类型的日志文件</p>
<ul>
<li>在配置文件settings.py中加上，LOG_LEVEL&#x3D;’ERROR’</li>
<li>查看指定错误的日志文件才显示</li>
</ul>
</li>
<li><p>设置不遵守爬虫协议</p>
<ul>
<li>在配置文件中ROBOTSTXT_OBEY 默认为true，遵守爬虫协议，更改为False</li>
</ul>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class SpidernameSpider(scrapy.Spider):</span><br><span class="line">    # 爬虫文件的名称：就是爬虫源文件的一个唯一标识</span><br><span class="line">    name = &#x27;spiderName&#x27;</span><br><span class="line">    # 允许的域名：用来限定start_url列表中那些url可以进行请求的发送，一般不使用</span><br><span class="line">    # allowed_domains = [&#x27;www.baidu.com&#x27;]</span><br><span class="line">    # 起始的url列表：该列表存放的url会被scrapy自动进行请求的发送</span><br><span class="line">    start_urls = [&#x27;https://ww.baidu.com/&#x27;,&#x27;https://www.sogou.com&#x27;]</span><br><span class="line"></span><br><span class="line">    # 用作于数据解析：response参数就是请求成功后对应的响应对象</span><br><span class="line">    def parse(self, response):</span><br><span class="line">        print(response)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="scrapy数据解析"><a href="#scrapy数据解析" class="headerlink" title="scrapy数据解析"></a>scrapy数据解析</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ShujclSpider(scrapy.Spider):</span><br><span class="line">    name = &#x27;shujcl&#x27;</span><br><span class="line">    # allowed_domains = [&#x27;www.baidu.com&#x27;]</span><br><span class="line">    start_urls = [&#x27;https://www.izuiyou.com/home&#x27;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        # 解析：作者的名称+段子内容</span><br><span class="line">        div_list=response.xpath(&#x27;//div[@class=&quot;Post&quot;]/div&#x27;)</span><br><span class="line">        for div in div_list:</span><br><span class="line">            # xpath返回的是列表，但是列表元素一定是Selector类型的对象</span><br><span class="line">            # extract可以将Selector对象中data参数存储的字符串提取出来</span><br><span class="line">            # extract_first提取第零个元素</span><br><span class="line">            	</span><br><span class="line">          author=div.xpath(&#x27;./div[1]/div[classs=&quot;Post__name&quot;]/text()&#x27;).extract_first()</span><br><span class="line">            # 列表调用extract之后，则表示将列表中每一个Selector对象中的data对应的字符串提取出来</span><br><span class="line">          content=div.xpath(&#x27;./div[2]/div[class=&quot;Post__container&quot;]//text()&#x27;).extract()</span><br><span class="line">            print(author,content)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="scrapy持久化存储"><a href="#scrapy持久化存储" class="headerlink" title="scrapy持久化存储"></a>scrapy持久化存储</h3><h4 id="基于终端指令"><a href="#基于终端指令" class="headerlink" title="基于终端指令"></a>基于终端指令</h4><ul>
<li>要求：只可以将parse方法的返回值存储到本地的文本文件中</li>
<li>指令：scrapy crawl [爬虫文件名] -o [文件路径]</li>
<li>这种持久化存储方式只能存储[‘json’, ‘jsonlines’, ‘jl’, ‘csv’, ‘xml’, ‘marshal’, ‘pickle’]文件格式</li>
<li>优点：简介高效便捷</li>
<li>缺点：局限性比较强（数据只能存储到指定后缀文件中）</li>
</ul>
<h4 id="基于管道"><a href="#基于管道" class="headerlink" title="基于管道"></a>基于管道</h4><ul>
<li><p>编码流程：</p>
<ol>
<li><p>数据解析</p>
</li>
<li><p>在item类中定义相关属性(items.py文件中的类中用name&#x3D;scrapy.Field()定义属性)</p>
</li>
<li><p>将解析的数据封装存储到item类型的对象</p>
</li>
<li><p>将item类型的对象交给管道进行持久化存储的操作（pipelines.py文件）</p>
</li>
<li><p>在管道类的process_item方法中要将其接受到的item对象中存储的数据进行持久化存储操作</p>
</li>
<li><p>在配置文件中开启管道(ITEM_PIPELINES &#x3D; {<br>‘shuju.pipelines.ShujuPipeline’: 300,<br>}注释解除)</p>
</li>
</ol>
<ul>
<li><p>数值300是优先级，数值越小优先级越高</p>
</li>
<li><p>好处：通用性强</p>
</li>
<li><p>管道文件中一个管道类对应将一组数据存储到一个平台或载体中,在管道文件中加入一个管道类在设置文件中也要相应加入管道类设置</p>
</li>
<li><p><span style="color:red">在管道文件的process_item方法中最好带上返回值，优先级别高的管道类接收item数据后会把返回值传递给下一个管道类</span></p>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;爬虫文件&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> zuiyou.items <span class="keyword">import</span> ZuiyouItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ShujclSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;shujcl&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.baidu.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.izuiyou.com/home&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 解析：作者的名称+段子内容</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;Post&quot;]/div&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            <span class="comment"># xpath返回的是列表，但是列表元素一定是Selector类型的对象</span></span><br><span class="line">            <span class="comment"># extract可以将Selector对象中data参数存储的字符串提取出来</span></span><br><span class="line">            <span class="comment"># extract_first提取第零个元素</span></span><br><span class="line"></span><br><span class="line">            author = div.xpath(<span class="string">&#x27;./div[1]/div[classs=&quot;Post__name&quot;]/text()&#x27;</span>).extract_first()</span><br><span class="line">            <span class="comment"># 列表调用extract之后，则表示将列表中每一个Selector对象中的data对应的字符串提取出来</span></span><br><span class="line">            content = div.xpath(<span class="string">&#x27;./div[2]/div[class=&quot;Post__container&quot;]//text()&#x27;</span>).extract()</span><br><span class="line">            <span class="comment"># 创建item类</span></span><br><span class="line">            zui = ZuiyouItem()</span><br><span class="line">            <span class="comment"># 必须使用中括号形式访问item类中定义的属性</span></span><br><span class="line">            zui[<span class="string">&#x27;author&#x27;</span>] = author</span><br><span class="line">            zui[<span class="string">&#x27;content&#x27;</span>] = content</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> zui <span class="comment"># 表示将item类提交给管道</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;items文件&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ZuiyouItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    author = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;pipelines文件&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ZuiyouPipeline</span>:</span><br><span class="line">    fp = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 重写父类的一个方法：该方法只在开始爬虫的时候被调用一次</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;开始爬虫。。。。。。。。&#x27;</span>)</span><br><span class="line">        self.fp = <span class="built_in">open</span>(<span class="string">&#x27;./zuiyou.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line">    <span class="comment"># 专门用来处理item类型对象</span></span><br><span class="line">    <span class="comment"># 该方法可以接受爬虫文件提交过来的item对象</span></span><br><span class="line">    <span class="comment"># 该方法每接收到一个item就会被调用一次</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        author = item[<span class="string">&#x27;author&#x27;</span>]</span><br><span class="line">        content = item[<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        self.fp.write(author + <span class="string">&#x27;:&#x27;</span> + content + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item <span class="comment"># 会把返回值交给下一个即将执行的管道类 </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 只在爬虫结束的时候调用一次</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;结束爬虫！！！&quot;</span>)</span><br><span class="line">        self.fp.close()</span><br></pre></td></tr></table></figure>

<h3 id="全站数据爬取"><a href="#全站数据爬取" class="headerlink" title="全站数据爬取"></a>全站数据爬取</h3><ul>
<li>对一个网站所有的页码对应的页面数据进行爬取</li>
<li>实现方式：<ol>
<li>将所有页码url添加到start_urls列表(比较呆板)</li>
<li>自动手动进行请求发送(推荐)</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> zuiyou.items <span class="keyword">import</span> ZuiyouItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ShujclSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;shujcl&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.baidu.com&#x27;]</span></span><br><span class="line">    <span class="comment"># 第一页数据</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.izuiyou.com/home&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成一个通用爬虫模块</span></span><br><span class="line">    url = <span class="string">&#x27;https://www.izuiyou.com/home/%d&#x27;</span></span><br><span class="line">    <span class="comment"># 第二页开始手动爬取</span></span><br><span class="line">    page_num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 解析：作者的名称+段子内容</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;Post&quot;]/div&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line"></span><br><span class="line">            author = div.xpath(<span class="string">&#x27;./div[1]/div[classs=&quot;Post__name&quot;]/text()&#x27;</span>).extract_first()</span><br><span class="line">            content = div.xpath(<span class="string">&#x27;./div[2]/div[class=&quot;Post__container&quot;]//text()&#x27;</span>).extract()</span><br><span class="line">        <span class="keyword">if</span> page_num &lt;= <span class="number">11</span>:    </span><br><span class="line">            new_uel = <span class="built_in">format</span>(self.url%self.page_num)    </span><br><span class="line">            self.page_num += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 进行手动爬取，并在成功获取响应对象时调用parse方法进行数据解析</span></span><br><span class="line">            <span class="comment"># callback主要是用来作数据解析</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=new_uel,callback=self.parse)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="五大核心组件"><a href="#五大核心组件" class="headerlink" title="五大核心组件"></a>五大核心组件</h3><ul>
<li><p>调度器(包含过滤器，队列)</p>
<ul>
<li>使用过滤器对url进行去重</li>
<li>用来接受引擎过来的请求，例如队列中，并在引擎再次请求的时候返回，可以想象成一个url(抓取网页的网址)的优先队列，由它来决定下一个要抓取的网址是什么，同时取出重复网址</li>
</ul>
</li>
<li><p>管道</p>
<ul>
<li>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体，验证实体的有效性，清楚不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并进过几个特定的次序处理数据</li>
</ul>
</li>
<li><p>引擎</p>
<ul>
<li>用来处理整个系统的数据流处理，触发事物(框架核心)</li>
</ul>
</li>
<li><p>下载器</p>
<ul>
<li>用于下载网页内容，并将网页内容返回给蜘蛛(scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
</ul>
</li>
<li><p>spider</p>
<ol>
<li><p>产生url</p>
</li>
<li><p>进行数据解析</p>
</li>
<li><p>爬虫是主要干活的，用于从特定的网页中提取自己需要的信息，即所谓的实体(item)。用户也可以从中提取出链接，让scrapy继续抓取下一个页面</p>
</li>
</ol>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="D://i/2023/03/05/image-20220808153912625.png" alt="image-20220808153912625"></p>
<h3 id="请求传参"><a href="#请求传参" class="headerlink" title="请求传参"></a>请求传参</h3><ul>
<li>使用场景：<ul>
<li>如果要爬取解析的数据不在同一张页面中。(深度爬取)</li>
<li>传递参数传递的是item对象</li>
<li>在Request函数中使用meta参数进行参数传递<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">该例子为：</span></span><br><span class="line"><span class="string">	爬取猎聘网站的python工作信息</span></span><br><span class="line"><span class="string">	且爬取详情页的工作具体描述</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> lieping.items <span class="keyword">import</span> LiepingItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LieurlSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;lieurl&#x27;</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.liepin.com/zhaopin/?inputFrom=www_index&amp;workYearCode=0&amp;key=python &#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_url</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 回调函数接收参数</span></span><br><span class="line">        item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">        job_desc = response.xpath(<span class="string">&#x27;/html/body/main/content/section[2]/dl/dd/text()&#x27;</span>).extract_first()</span><br><span class="line">        item[<span class="string">&#x27;job_desc&#x27;</span>] = job_desc</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        name = response.xpath(<span class="string">&#x27;//div[@class=&quot;job-title-box&quot;]/div[1]/@title&#x27;</span>).extract()</span><br><span class="line">        url = response.xpath(<span class="string">&#x27;//div[@class=&quot;job-detail-box&quot;]/a[1]/@href&#x27;</span>).extract()</span><br><span class="line">        <span class="built_in">print</span>(response)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(name)):</span><br><span class="line">            item = LiepingItem()</span><br><span class="line">            item[<span class="string">&#x27;name&#x27;</span>] = name[i]</span><br><span class="line">            new_url = url[i].split(<span class="string">&#x27;&amp;&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 请求传参</span></span><br><span class="line">            <span class="comment"># 在请求方法中，使用meta=&#123;&#125;参数，可以将meta字典传递给请求对应的回调函数</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(new_url, callback=self.parse_url, meta=&#123;<span class="string">&#x27;item&#x27;</span>: item&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="图片爬取值ImagesPipeline类"><a href="#图片爬取值ImagesPipeline类" class="headerlink" title="图片爬取值ImagesPipeline类"></a>图片爬取值ImagesPipeline类</h3><ul>
<li><p>基于scrapy爬取字符串类型的数据和爬取图片类型的数据区别：</p>
<ul>
<li>字符串：只需要基于xpath今夕解析且提交管道进行持久化存储</li>
<li>图片：xpath解析除图片src的属性值。单独的对图片地址发起请求获取图片二进制类型的数据</li>
</ul>
</li>
<li><p>基于ImagesPipeline类：</p>
<ul>
<li>只需要将img的src的属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制类型的数据，还会进行持久化存储</li>
</ul>
</li>
<li><p>需求：爬取二次元网站的cos图片</p>
</li>
<li><p>使用流程：</p>
<ol>
<li><p>数据解析(图片的地址)</p>
</li>
<li><p>将存储图片地址的item提交到制定的管道类</p>
</li>
<li><p>在管道文件中自定一个基于ImagesPipeLine的一个管道类,并重写三个方法</p>
<ol>
<li><p>get_media_requests:根据图片地址进行图片数据的请求</p>
</li>
<li><p>file_path:指定图片的存储路径</p>
</li>
<li><p>item_completed:把item返回给下一个即将被执行的管道类</p>
</li>
</ol>
</li>
<li><p>在配置文件中配置图片下载路径</p>
</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 爬虫文件</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> erciyuan.items <span class="keyword">import</span> ErciyuanItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ErciSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;erci&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;t2cy.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://t2cy.com/acg/cos/cosplay/2022-07-25/1569.html&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="built_in">print</span>(response)</span><br><span class="line">        p_list = response.xpath(<span class="string">&#x27;/html/body/div/div[1]/div[1]/div[2]/p&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> p_list:</span><br><span class="line">            <span class="comment"># 该网站使用了图片懒加载</span></span><br><span class="line">            <span class="comment"># 所以除了src还要获取data-loadsrc属性</span></span><br><span class="line">            src = <span class="string">&#x27;https://t2cy.com&#x27;</span>+p.xpath(<span class="string">&#x27;./img/@src | ./img/@data-loadsrc&#x27;</span>).extract_first()</span><br><span class="line">            item = ErciyuanItem()</span><br><span class="line">            item[<span class="string">&#x27;src&#x27;</span>] = src</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 管道文件</span></span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">imagePileLine</span>(<span class="title class_ inherited__">ImagesPipeline</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可以根据图片地址进行图片数据的请求</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_media_requests</span>(<span class="params">self, item, info</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(item[<span class="string">&#x27;src&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 指定图片进行持久化存储的路径</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">file_path</span>(<span class="params">self, request, response=<span class="literal">None</span>, info=<span class="literal">None</span>, *, item=<span class="literal">None</span></span>):</span><br><span class="line">        image_name = request.url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> image_name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">item_completed</span>(<span class="params">self, results, item, info</span>):</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<h3 id="中间件"><a href="#中间件" class="headerlink" title="中间件"></a>中间件</h3><p>主要有爬虫中间件与下载中间件(重点)</p>
<p>下载中间件：处于下载器和引擎之间。</p>
<ul>
<li><p>作用：可以批量拦截整个工程中发起的所有的的请求和响应</p>
</li>
<li><p>拦截请求：</p>
<ul>
<li>UA伪装</li>
<li>代理IP</li>
</ul>
</li>
<li><p>拦截响应：</p>
<ul>
<li>篡改响应数据，响应对象</li>
</ul>
</li>
<li><p><code>使用中间件记得在设置文件中开启对应的设置</code></p>
</li>
<li><p>拦截请求</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">需求：爬取百度ip搜索页</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 爬虫文件</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LanjieSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;lanjie&#x27;</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.baidu.com/s?wd=ip&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="built_in">print</span>(response)</span><br><span class="line">        page_text = response.text</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;ip.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            fp.write(page_text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># middlewars文件</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> is_item, ItemAdapter</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QqiuDownloaderMiddleware</span>:</span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the downloader middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line">    <span class="comment"># 封装一个UA池</span></span><br><span class="line">    user_agent_list = [</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 &quot;</span></span><br><span class="line">        <span class="string">&quot;(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24&quot;</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 代理池</span></span><br><span class="line">    PROXY_http = [</span><br><span class="line">        <span class="string">&#x27;153.180.102.104:80&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;195.208.131.189:56055&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line">    PROXY_https = [</span><br><span class="line">        <span class="string">&#x27;120.83.49.90:9000&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;95.189.112.214:35508&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 拦截正常请求</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># UA伪装</span></span><br><span class="line">        request.headers[<span class="string">&#x27;User-Agent&#x27;</span>] = random.choice(self.user_agent_list)</span><br><span class="line">        <span class="comment"># 代理ip设置，为了验证，一般卸载process_exception</span></span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&#x27;http://47.113.90.161&#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拦截所有的响应</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="comment"># Called with the response returned from the downloader.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either;</span></span><br><span class="line">        <span class="comment"># - return a Response object</span></span><br><span class="line">        <span class="comment"># - return a Request object</span></span><br><span class="line">        <span class="comment"># - or raise IgnoreRequest</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拦截发生异常的请求</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="comment"># 代理</span></span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&#x27;http://&#x27;</span> + random.choice(self.PROXY_http) <span class="keyword">if</span> request.url.split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">0</span>] == <span class="string">&#x27;http&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;https://&#x27;</span> + random.choice(self.PROXY_https)</span><br><span class="line">        <span class="keyword">return</span> request <span class="comment"># 将修正之后的请求对象进行重新的请求发送</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>拦截响应对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">需求：爬取网易新闻中的新闻数据(标题和内容)</span></span><br><span class="line"><span class="string">    1.通过网易新闻的首页解析出四大板块对应的详情页的url</span></span><br><span class="line"><span class="string">    2.每一个板块对应的新闻标题都是动态加载出来的</span></span><br><span class="line"><span class="string">    3.通过解析除每一天新闻详情页的url获取详情页的页面源码，解析出新闻内容</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 爬虫文件</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> wanyipro.items <span class="keyword">import</span> WanyiproItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WanyiSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;wanyi&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;news.163.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;https://news.163.com/&#x27;</span>]</span><br><span class="line">    moders_urls = [] <span class="comment"># 存储各个板块的url</span></span><br><span class="line">    <span class="comment"># 实例化浏览器对象</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.bro = webdriver.Edge(<span class="string">&#x27;爬虫/msedgedriver.exe&#x27;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;index2016_wrap&quot;]/div[2]/div[2]/div[2]/div[2]/div/ul/li&#x27;</span>)</span><br><span class="line">        a_list = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> a_list:</span><br><span class="line">            model_url = li_list[index].xpath(<span class="string">&#x27;./a/@href&#x27;</span>).extract_first()</span><br><span class="line">            self.moders_urls.append(model_url)</span><br><span class="line">        <span class="comment"># 依次对每个板块的页面进行请求</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.moders_urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url,callback=self.parse_model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析每一个板块页面对应新闻的标题和详情页的url</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_model</span>(<span class="params">self,response</span>):</span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;/html/body/div/div[3]/div[3]/div[1]/div[1]/div/ul/li/div/div[3]&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            title = div.xpath(<span class="string">&#x27;.//h3/a/text()&#x27;</span>).extract_first()</span><br><span class="line">            new_detail_url = div.xpath(<span class="string">&#x27;.//h3/a/@href&#x27;</span>).extract_first()</span><br><span class="line">            item = WanyiproItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 新闻详情页的url发起请求</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(new_detail_url,callback=self.parse_detail,meta=&#123;<span class="string">&#x27;item&#x27;</span>:item&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">self,response</span>):</span><br><span class="line">        item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">        content = response.xpath(<span class="string">&#x27;//*[@id=&quot;content&quot;]/div[2]//text()&#x27;</span>).extract()</span><br><span class="line">        content = <span class="string">&#x27;&#x27;</span>.join(content)</span><br><span class="line">        item[<span class="string">&#x27;content&#x27;</span>] = content</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写父类方法，只在爬虫关闭时调用一次</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">closed</span>(<span class="params">self,spider</span>):</span><br><span class="line">        self.bro.quit()</span><br><span class="line">        </span><br><span class="line"><span class="comment"># middlewares文件</span></span><br><span class="line"><span class="comment"># 拦截响应对象，进行篡改</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        bro = spider.bro</span><br><span class="line">        <span class="comment"># 挑选出指定的响应对象进行篡改</span></span><br><span class="line">        <span class="comment"># 通过url指定request</span></span><br><span class="line">        <span class="comment"># 通过request指定的response</span></span><br><span class="line">        <span class="keyword">if</span> request.url <span class="keyword">in</span> spider.models_urls:</span><br><span class="line">            bro.get(request.url)</span><br><span class="line">            sleep(<span class="number">2</span>)</span><br><span class="line">            page_text = bro.page_source</span><br><span class="line">            <span class="comment"># 实例化一个新的响应对象，替代原来不满足需求的响应对象</span></span><br><span class="line">            <span class="comment"># 基于selenium便捷的获取动态加载数据</span></span><br><span class="line">            <span class="comment"># 把selenium和scrapy进行结合</span></span><br><span class="line">            new_response = HtmlResponse(url=request.url, body=page_text, encoding=<span class="string">&#x27;utf8&#x27;</span>, request=request)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> new_response <span class="comment"># 替换旧的response</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="CrawlSpider类"><a href="#CrawlSpider类" class="headerlink" title="CrawlSpider类"></a>CrawlSpider类</h3><p>CrawlSpider类：spider的子类，专门用作全站数据的爬取</p>
<ul>
<li><p>全站数据爬取方式：</p>
<ol>
<li>基于spider：手动请求</li>
<li>基于crawlspider</li>
</ol>
</li>
<li><p>crawl的使用：只有创建爬虫文件时有所不同</p>
<ul>
<li><p>scrapy genspider -t crawl 文件名 url</p>
</li>
<li><p>链接提取器(LinkExtractor)：根据指定规则(allow&#x3D;’正则’)进行指定链接的提取</p>
</li>
<li><p>规则解析器(Rule)：将链接提取器提取到的链接进行指定规则的解析操作</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">需求：爬取全部页码的古诗文网的诗词题目和详情页的内容</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 爬虫文件</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> sunpro.items <span class="keyword">import</span> SunproItem,ContentItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SunSpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;sun&#x27;</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://so.gushiwen.cn/shiwens/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># 规则解析器</span></span><br><span class="line">        <span class="comment"># LinkExtractor链接提取器</span></span><br><span class="line">        <span class="comment"># follow=True：可以将链接提取器继续作用到链接提取器提取到的链接所对应的页面中</span></span><br><span class="line">        <span class="comment"># 使用follow会产生大量重复链接，调度器的过滤器会去重</span></span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;page=\d+&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;/shiwenv\w+\.aspx&#x27;</span>), callback=<span class="string">&#x27;parse_detail&#x27;</span>,follow=<span class="literal">False</span>),</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 如下两个解析方法中是不可以实现请求传参的</span></span><br><span class="line">    <span class="comment"># 无法将两个解析方法解析的数据存储到一个item中，可以存储到两个item</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 注意：在xpath表达式中不可以出现tbody标签</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;leftZhankai&quot;]/div[@class=&quot;sons&quot;]&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            title = div.xpath(<span class="string">&#x27;./div[1]//p//text()&#x27;</span>).extract_first()</span><br><span class="line">            item = SunproItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析详情页内容</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">self,response</span>):</span><br><span class="line">        content = response.xpath(<span class="string">&#x27;//*[@class=&quot;contson&quot;]/text()&#x27;</span>).extract_first()</span><br><span class="line">        item = ContentItem()</span><br><span class="line">        item[<span class="string">&#x27;content&#x27;</span>] = content</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 管道文件</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SunproPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="comment"># 如何判定item类型</span></span><br><span class="line">        <span class="keyword">if</span> item.__class__.__name__ == <span class="string">&#x27;SunproItem&#x27;</span>:</span><br><span class="line">            <span class="built_in">print</span>(item[<span class="string">&#x27;title&#x27;</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(item[<span class="string">&#x27;content&#x27;</span>])</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="scrapy设置文件"><a href="#scrapy设置文件" class="headerlink" title="scrapy设置文件"></a>scrapy设置文件</h3><ol>
<li><p>设置日志等级</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">LOG_LEVEL = &#x27;ERROR&#x27;</span><br><span class="line"># 只输出ERROR日志信息</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置是否遵守爬虫协议</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = False</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置开启管道</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">&#x27;shuju.pipelines.ShujuPipeline&#x27;: 300,</span><br><span class="line">&#125;</span><br><span class="line"># 在设置文件中解除注释</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置图片存储路径</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 图片会都存再imgs文件夹中</span><br><span class="line"># 会自行创建</span><br><span class="line">IMAGES_STORE = &#x27;./imgs&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置开启下载中间件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在设置文件中把这解除注释</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">&#x27;qqiu.middlewares.QqiuDownloaderMiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="分布式爬虫"><a href="#分布式爬虫" class="headerlink" title="分布式爬虫"></a>分布式爬虫</h2><ul>
<li><p>概念：需要搭建一个分布式的机群，让其对一组资源进行分布联合爬取</p>
</li>
<li><p>作用：提升爬取数据的效率</p>
</li>
<li><p>如何实现分布式？</p>
<ul>
<li><p>安装scrapy-redis的组件</p>
</li>
<li><p>原生的scrapy不可以实现分布式爬虫，必须要让scrapy结合scrapy-redis组件一起实现分布式爬虫</p>
<ul>
<li>调度器不可以被分布式机群共享</li>
<li>管道不可以被分布式机群共享</li>
</ul>
</li>
<li><p>scrapy-redis组件作用：</p>
<ul>
<li>可以给原生的scrapy框架提供可以被共享的管道和调度器</li>
</ul>
</li>
<li><p>实现流程</p>
<ol>
<li><p>创建一个工程</p>
</li>
<li><p>创建一个基于CrawlSpider的爬虫文件</p>
</li>
<li><p>修改当前爬虫文件：</p>
<ol>
<li>导包：from scrapy_redis.spider import RedisCrawlSpider，并且把爬虫类的父类改成RedisCrawlSpider</li>
<li>将start_urls和allowed_domains进行注释</li>
<li>添加redis_key &#x3D; ‘baidu’属性，为可以被共享的调度器队列的名称</li>
<li>编写数据解析相关操作</li>
</ol>
</li>
<li><p>修改配置文件</p>
<ol>
<li><p>指定可以被共享的管道</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">	&#x27;scrapy_reids.pipelines.RedisPipeline&#x27;:400</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>指定调度器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 增加一个去重容器类的配置，作用使用Redis的set集合来存储请求的指纹数据，从而实现请求去重的持久化存储</span><br><span class="line">DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line"># 使用scrapy_redis组件自己的调度器</span><br><span class="line">SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"># 配置调度器是否要持久化，也就是当爬虫结束了，要不要清空Redis中请求队列和去重指纹的set</span><br><span class="line">SCHEDULER_PERSIST = True</span><br></pre></td></tr></table></figure>
</li>
<li><p>指定redis服务器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 指定redis数据库  </span><br><span class="line">REDIS_HOST = &#x27;xxx.xxx.xxx.xxx&#x27;</span><br><span class="line">REDIS_PORT = xxx</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>redis相关操作配置</p>
<ol>
<li>配置redis的配置文件<ol>
<li>把默认绑定bind注释掉：否则只能自己电脑能访问</li>
<li>关闭保护模式：protected-mode：no：否则其他电脑访问只能读数据，不能写数据</li>
</ol>
</li>
<li>结合配置文件开启redis服务<ol>
<li>redis-server 配置文件</li>
</ol>
</li>
<li>启动客户端<ol>
<li>redis-cli</li>
</ol>
</li>
</ol>
</li>
<li><p>执行工程：scrapy runspider [爬虫源文件名称]</p>
</li>
<li><p>向调度器的队列中放入起始url</p>
<ul>
<li>调度器的队列在redis的客户端中<ul>
<li>lpush [队列名称] [起始URL]</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 爬虫文件</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span>  Rule</span><br><span class="line"><span class="keyword">from</span> scrapy_redis.spiders <span class="keyword">import</span> RedisCrawlSpider</span><br><span class="line"><span class="keyword">from</span> fenbu.items <span class="keyword">import</span> FenbuItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FbsSpider</span>(<span class="title class_ inherited__">RedisCrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;fbs&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xx.com&#x27;]</span></span><br><span class="line">    <span class="comment"># start_urls = [&#x27;http://www.xx.com/&#x27;]</span></span><br><span class="line">    <span class="comment"># 可以被共享的调度器队列的名称</span></span><br><span class="line">    redis_key = <span class="string">&#x27;baidu&#x27;</span></span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># 规则解析器</span></span><br><span class="line">        <span class="comment"># LinkExtractor链接提取器</span></span><br><span class="line">        <span class="comment"># follow=True：可以将链接提取器继续作用到链接提取器提取到的链接所对应的页面中</span></span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;page=\d+&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 注意：在xpath表达式中不可以出现tbody标签</span></span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;leftZhankai&quot;]/div[@class=&quot;sons&quot;]&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            title = div.xpath(<span class="string">&#x27;./div[1]//p//text()&#x27;</span>).extract_first()</span><br><span class="line">            item = FenbuItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = title</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">其实代码编写步骤和scrapy框架差不多</span></span><br><span class="line"><span class="string">很多都是配置上的步骤和redis的知识</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="关于开发者工具与页面源代码不同"><a href="#关于开发者工具与页面源代码不同" class="headerlink" title="关于开发者工具与页面源代码不同"></a>关于开发者工具与页面源代码不同</h2><ul>
<li>有的网站反爬会把某些数据动态加载，这时候我们爬不到自己想要的页面，这时候我们可以读取页面源代码获取我们想要的数据</li>
<li>可以通过抓包工具判断是否为动态加载的数据,再决定是否使用selenium模块获取动态数据也可以</li>
</ul>
<h2 id="关于一次获取多个xpath数据"><a href="#关于一次获取多个xpath数据" class="headerlink" title="关于一次获取多个xpath数据"></a>关于一次获取多个xpath数据</h2><ul>
<li>xpath语句中可以用’|’符号分隔以获取多个xpath语句的属性</li>
</ul>
<h2 id="关于中文乱码解决方案"><a href="#关于中文乱码解决方案" class="headerlink" title="关于中文乱码解决方案"></a>关于中文乱码解决方案</h2><h2 id="多页面爬取"><a href="#多页面爬取" class="headerlink" title="多页面爬取"></a>多页面爬取</h2><ul>
<li>post请求：在参数中有页数参数，通过修改可以改变页面</li>
<li>get请求：url参数中有页面参数，通过修改可以改变页面</li>
</ul>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">AYO</div><div class="post-copyright__author_desc">天行健，君子以自强不息</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://www.yuki.love/2021/09/09/MarkDown/python/%E7%88%AC%E8%99%AB/%E8%81%9A%E7%84%A6%E5%BC%8F%E7%88%AC%E8%99%AB/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://www.yuki.love/2021/09/09/MarkDown/python/%E7%88%AC%E8%99%AB/%E8%81%9A%E7%84%A6%E5%BC%8F%E7%88%AC%E8%99%AB/')">聚焦式爬虫</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://www.yuki.love/2021/09/09/MarkDown/python/%E7%88%AC%E8%99%AB/%E8%81%9A%E7%84%A6%E5%BC%8F%E7%88%AC%E8%99%AB/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=聚焦式爬虫&amp;url=https://www.yuki.love/2021/09/09/MarkDown/python/%E7%88%AC%E8%99%AB/%E8%81%9A%E7%84%A6%E5%BC%8F%E7%88%AC%E8%99%AB/&amp;pic=https://th.bing.com/th/id/R.4cb70989991c1861426358cb196b6d01?rik=83OmAlSQSFM51Q&amp;riu=http%3A%2F%2Fpic.bizhi360.com%2Fbpic%2F88%2F9588_8.jpg&amp;ehk=Vc5LFstc1coz3liGA9YtX3SOl%2BcMJTlRfHNhk%2F4%2FA0w%3D&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;_r_=a160cf65-bb3d-5a5b-fd48-403e56c22880" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.yuki.love" target="_blank">AYO</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/Python/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>Python<span class="tagsPageCount">8</span></a><a class="post-meta__box__tags" href="/tags/%E5%BC%80%E5%8F%91/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>开发<span class="tagsPageCount">7</span></a><a class="post-meta__box__tags" href="/tags/%E7%88%AC%E8%99%AB/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>爬虫<span class="tagsPageCount">4</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://th.bing.com/th/id/OIP.IV7LvR2MoBb3y-EKG9uAJAHaEK?rs=1&amp;pid=ImgDetMain&amp;_r_=bfbcb5ad-d0e2-9fe7-98ba-3bfeb2fa0a7a" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/08/17/MarkDown/python/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%88%9D%E5%A7%8B/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://pic2.zhimg.com/v2-264805954c1f033af43e006cb826ab60_r.jpg?source=1940ef5c&amp;_r_=636d9635-4810-fa03-bf3c-273891ec2cd7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">爬虫基础</div></div></a></div><div class="next-post pull-right"><a href="/2021/09/09/MarkDown/python/%E7%88%AC%E8%99%AB/%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://pic1.zhimg.com/v2-37d1dd62d373427b8035891e9fe1653e_r.jpg?source=1940ef5c&amp;_r_=51a6a199-d62b-688a-b834-11678aa494c9" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">增量式爬虫</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/2021/09/09/MarkDown/python/%E7%88%AC%E8%99%AB/%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB/" title="增量式爬虫"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://pic1.zhimg.com/v2-37d1dd62d373427b8035891e9fe1653e_r.jpg?source=1940ef5c&_r_=51a6a199-d62b-688a-b834-11678aa494c9" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2021-09-09</div><div class="title">增量式爬虫</div></div></a></div><div><a href="/2021/08/17/MarkDown/python/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%88%9D%E5%A7%8B/" title="爬虫基础"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://pic2.zhimg.com/v2-264805954c1f033af43e006cb826ab60_r.jpg?source=1940ef5c&_r_=636d9635-4810-fa03-bf3c-273891ec2cd7" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2021-08-17</div><div class="title">爬虫基础</div></div></a></div><div><a href="/2021/08/17/MarkDown/python/%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E5%AF%BC%E8%AE%BA/" title="爬虫介绍"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://th.bing.com/th/id/R.d8dfd08893b58d08d74b38ad8870a48d?rik=9KBqff6Rai035Q&riu=http%3A%2F%2Fstatic.cntonan.com%2Fuploadfile%2F2019%2F0214%2F20190214104244pwm1xxsdikh.jpg&ehk=MOxI2n5nY44gO%2FKsNYWAuEBvcRwSmkRVNb4dTS6Gk%2BY%3D&risl=&pid=ImgRaw&r=0&_r_=4e1c5bfb-bd9b-b1b0-b972-ddd5d820ff34" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2021-08-17</div><div class="title">爬虫介绍</div></div></a></div><div><a href="/2021/06/15/MarkDown/python/python%E5%9F%BA%E7%A1%80/python%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/" title="Python中的垃圾回收机制"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://pic2.zhimg.com/v2-264805954c1f033af43e006cb826ab60_r.jpg?source=1940ef5c&_r_=62470469-721b-9664-2ed5-d7e52525c995" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2021-06-15</div><div class="title">Python中的垃圾回收机制</div></div></a></div><div><a href="/2021/04/04/MarkDown/python/python%E5%9F%BA%E7%A1%80/%E5%B8%B8%E7%94%A8%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/" title="Python内置函数"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://th.bing.com/th/id/OIP.IV7LvR2MoBb3y-EKG9uAJAHaEK?rs=1&pid=ImgDetMain&_r_=44ac69ee-8e13-a63a-b234-54568f4f84c4" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2021-04-04</div><div class="title">Python内置函数</div></div></a></div><div><a href="/2021/02/15/MarkDown/python/python%E5%9F%BA%E7%A1%80/python%E5%9F%BA%E7%A1%80/" title="Python"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://th.bing.com/th/id/R.d8dfd08893b58d08d74b38ad8870a48d?rik=9KBqff6Rai035Q&riu=http%3A%2F%2Fstatic.cntonan.com%2Fuploadfile%2F2019%2F0214%2F20190214104244pwm1xxsdikh.jpg&ehk=MOxI2n5nY44gO%2FKsNYWAuEBvcRwSmkRVNb4dTS6Gk%2BY%3D&risl=&pid=ImgRaw&r=0&_r_=cb55c179-26bb-e701-2045-da19661ad9e7" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2021-02-15</div><div class="title">Python</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="anzhiyufont anzhiyu-icon-comments"></i><span> 评论</span></div><div class="comment-randomInfo"><a onclick="anzhiyu.addRandomCommentInfo()" href="javascript:void(0)">匿名评论</a><a href="/privacy" style="margin-left: 4px">隐私政策</a></div><div class="comment-switch"><span class="first-comment">Twikoo</span><span id="switch-btn"></span><span class="second-comment">Waline</span></div><div class="comment-tips" id="comment-tips"><span>✅ 你无需删除空行，直接评论以获取最佳展示效果</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div id="waline-wrap"></div></div></div></div><div class="comment-barrage"></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/24/64e6ce9c507bb.png" alt="status"/></div></div><div class="author-info__description"><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);"><b style="color:#fff">AYO的知识输出基地，</b><b style="color:#fff">以输出倒逼输入，以输出巩固知识！</b></div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/"><h1 class="author-info__name">AYO</h1><div class="author-info__desc">天行健，君子以自强不息</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/AYO-Al" target="_blank" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="social-icon faa-parent animated-hover" href="https://juejin.cn/user/3708003152300183" target="_blank" title="JueJin"><i class="anzhiyufont anzhiyu-icon-book"></i></a></div></div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E7%84%A6%E7%88%AC%E8%99%AB"><span class="toc-number">1.</span> <span class="toc-text">聚焦爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="toc-number">2.</span> <span class="toc-text">1. 数据解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E5%88%86%E7%B1%BB"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 数据解析分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E5%8E%9F%E7%90%86"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 数据解析原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%AD%A3%E5%88%99%E8%A7%A3%E6%9E%90"><span class="toc-number">3.</span> <span class="toc-text">2. 正则解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 正则表达式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%B8%B8%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 常用正则表达式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%AE%9E%E6%88%98%E4%B9%8B%E6%9A%B4%E8%B5%B0%E6%BC%AB%E7%94%BB"><span class="toc-number">3.3.</span> <span class="toc-text">2.2 实战之暴走漫画</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bs4%E8%A7%A3%E6%9E%90"><span class="toc-number">4.</span> <span class="toc-text">bs4解析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#xpath%E8%A7%A3%E6%9E%90"><span class="toc-number">5.</span> <span class="toc-text">xpath解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#xpath%E5%AE%9E%E6%88%98%E4%B9%8B%E7%88%AC%E5%8F%9658%E4%BA%8C%E6%89%8B%E6%88%BF"><span class="toc-number">5.1.</span> <span class="toc-text">xpath实战之爬取58二手房</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#xpath%E5%AE%9E%E6%88%98%E4%B9%8B4k%E5%9B%BE%E7%89%87%E8%A7%A3%E6%9E%90%E4%B8%8B%E8%BD%BD"><span class="toc-number">5.2.</span> <span class="toc-text">xpath实战之4k图片解析下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#xpath%E5%AE%9E%E6%88%98%E4%B9%8B%E5%85%A8%E5%9B%BD%E5%9F%8E%E5%B8%82%E5%90%8D%E7%A7%B0%E7%88%AC%E5%8F%96"><span class="toc-number">5.3.</span> <span class="toc-text">xpath实战之全国城市名称爬取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#xpath%E5%AE%9E%E6%88%98%E4%B9%8B%E5%9B%BE%E7%89%87%E7%88%AC%E5%8F%96"><span class="toc-number">5.4.</span> <span class="toc-text">xpath实战之图片爬取</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%AB%99%E9%AA%8C%E8%AF%81%E7%A0%81"><span class="toc-number">6.</span> <span class="toc-text">网站验证码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B6%85%E7%BA%A7%E9%B9%B0%E4%BD%BF%E7%94%A8"><span class="toc-number">6.1.</span> <span class="toc-text">超级鹰使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95"><span class="toc-number">7.</span> <span class="toc-text">模拟登录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cookie%E6%93%8D%E4%BD%9C"><span class="toc-number">8.</span> <span class="toc-text">cookie操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%90%86%E7%90%86%E8%AE%BA"><span class="toc-number">9.</span> <span class="toc-text">代理理论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB"><span class="toc-number">10.</span> <span class="toc-text">异步爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#selenium%E6%A8%A1%E5%9D%97"><span class="toc-number">11.</span> <span class="toc-text">selenium模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scrapy%E6%A1%86%E6%9E%B6"><span class="toc-number">12.</span> <span class="toc-text">scrapy框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="toc-number">12.1.</span> <span class="toc-text">scrapy数据解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8"><span class="toc-number">12.2.</span> <span class="toc-text">scrapy持久化存储</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%BB%88%E7%AB%AF%E6%8C%87%E4%BB%A4"><span class="toc-number">12.2.1.</span> <span class="toc-text">基于终端指令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%AE%A1%E9%81%93"><span class="toc-number">12.2.2.</span> <span class="toc-text">基于管道</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E7%AB%99%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="toc-number">12.3.</span> <span class="toc-text">全站数据爬取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94%E5%A4%A7%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-number">12.4.</span> <span class="toc-text">五大核心组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%B7%E6%B1%82%E4%BC%A0%E5%8F%82"><span class="toc-number">12.5.</span> <span class="toc-text">请求传参</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E7%88%AC%E5%8F%96%E5%80%BCImagesPipeline%E7%B1%BB"><span class="toc-number">12.6.</span> <span class="toc-text">图片爬取值ImagesPipeline类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-number">12.7.</span> <span class="toc-text">中间件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CrawlSpider%E7%B1%BB"><span class="toc-number">12.8.</span> <span class="toc-text">CrawlSpider类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy%E8%AE%BE%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">12.9.</span> <span class="toc-text">scrapy设置文件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="toc-number">13.</span> <span class="toc-text">分布式爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E5%BC%80%E5%8F%91%E8%80%85%E5%B7%A5%E5%85%B7%E4%B8%8E%E9%A1%B5%E9%9D%A2%E6%BA%90%E4%BB%A3%E7%A0%81%E4%B8%8D%E5%90%8C"><span class="toc-number">14.</span> <span class="toc-text">关于开发者工具与页面源代码不同</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E4%B8%80%E6%AC%A1%E8%8E%B7%E5%8F%96%E5%A4%9A%E4%B8%AAxpath%E6%95%B0%E6%8D%AE"><span class="toc-number">15.</span> <span class="toc-text">关于一次获取多个xpath数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">16.</span> <span class="toc-text">关于中文乱码解决方案</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B5%E9%9D%A2%E7%88%AC%E5%8F%96"><span class="toc-number">17.</span> <span class="toc-text">多页面爬取</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/06/20/MarkDown/%E4%BA%91%E8%AE%A1%E7%AE%97/Linux/ELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0/" title="ELK"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://th.bing.com/th/id/OIP.IV7LvR2MoBb3y-EKG9uAJAHaEK?rs=1&amp;pid=ImgDetMain&amp;_r_=bfbcb5ad-d0e2-9fe7-98ba-3bfeb2fa0a7a" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ELK"/></a><div class="content"><a class="title" href="/2024/06/20/MarkDown/%E4%BA%91%E8%AE%A1%E7%AE%97/Linux/ELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0/" title="ELK">ELK</a><time datetime="2024-06-19T16:00:00.000Z" title="发表于 2024-06-20 00:00:00">2024-06-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/01/MarkDown/%E4%BA%91%E8%AE%A1%E7%AE%97/Nginx/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/" title="Nginx反向代理配置"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://pic2.zhimg.com/v2-264805954c1f033af43e006cb826ab60_r.jpg?source=1940ef5c&amp;_r_=832fb211-66f4-6533-3f4a-04fdaaf14b2f" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Nginx反向代理配置"/></a><div class="content"><a class="title" href="/2024/06/01/MarkDown/%E4%BA%91%E8%AE%A1%E7%AE%97/Nginx/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/" title="Nginx反向代理配置">Nginx反向代理配置</a><time datetime="2024-05-31T16:00:00.000Z" title="发表于 2024-06-01 00:00:00">2024-06-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/27/MarkDown/%E4%BA%91%E8%AE%A1%E7%AE%97/Nginx/Nginx-all/" title="Nginx模块"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://pic2.zhimg.com/v2-264805954c1f033af43e006cb826ab60_r.jpg?source=1940ef5c&amp;_r_=6db1f708-e426-8b7d-686b-2d171176ea94" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Nginx模块"/></a><div class="content"><a class="title" href="/2024/05/27/MarkDown/%E4%BA%91%E8%AE%A1%E7%AE%97/Nginx/Nginx-all/" title="Nginx模块">Nginx模块</a><time datetime="2024-05-26T16:00:00.000Z" title="发表于 2024-05-27 00:00:00">2024-05-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/22/MarkDown/%E4%BA%91%E8%AE%A1%E7%AE%97/Linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%8E%E7%A3%81%E7%9B%98/" title="Linux文件系统与磁盘"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://th.bing.com/th/id/OIP.IV7LvR2MoBb3y-EKG9uAJAHaEK?rs=1&amp;pid=ImgDetMain&amp;_r_=fc940a6e-80c7-1abf-5a2f-f2f364acc9a9" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux文件系统与磁盘"/></a><div class="content"><a class="title" href="/2024/05/22/MarkDown/%E4%BA%91%E8%AE%A1%E7%AE%97/Linux/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%8E%E7%A3%81%E7%9B%98/" title="Linux文件系统与磁盘">Linux文件系统与磁盘</a><time datetime="2024-05-21T16:00:00.000Z" title="发表于 2024-05-22 00:00:00">2024-05-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/11/MarkDown/%E4%BA%91%E8%AE%A1%E7%AE%97/Linux/%E9%98%B2%E7%81%AB%E5%A2%99/" title="Linux防火墙"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://pic4.zhimg.com/v2-c2b3eb3319d52218753d0ce20bc7c100_r.jpg?_r_=be59e589-6cf8-2c67-aedb-4ec74b3e3d19" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux防火墙"/></a><div class="content"><a class="title" href="/2024/05/11/MarkDown/%E4%BA%91%E8%AE%A1%E7%AE%97/Linux/%E9%98%B2%E7%81%AB%E5%A2%99/" title="Linux防火墙">Linux防火墙</a><time datetime="2024-05-10T16:00:00.000Z" title="发表于 2024-05-11 00:00:00">2024-05-11</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="footer_deal"><a class="deal_link" href="/2609320892@qq.com" title="email"><i class="anzhiyufont anzhiyu-icon-envelope"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://regexlearn.com/zh-cn" title="Regex 101"><i class="anzhiyufont anzhiyu-icon-font"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://regexcrossword.com/" title="Regexcrossword"><i class="anzhiyufont anzhiyu-icon-bolt"></i></a><a class="deal_link" href="/atom.xml" title="RSS"><i class="anzhiyufont anzhiyu-icon-rss"></i></a><img class="footer_mini_logo" title="返回顶部" alt="返回顶部" onclick="anzhiyu.scrollToDest(0, 500)" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" size="50px"/><a class="deal_link" target="_blank" rel="noopener" href="https://github.com/AYO-Al" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://learngitbranching.js.org/?demo=&amp;locale=zh_CN" title="Learn Git"><i class="anzhiyufont anzhiyu-icon-pencil"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://www.drawio.com/" title="draw.io"><i class="anzhiyufont anzhiyu-icon-instagram"></i></a><a class="deal_link" href="/copyright" title="CC"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i></a></div><div id="anzhiyu-footer"><div class="footer-group"><div class="footer-title">服务</div><div class="footer-links"><a class="footer-item" title="algolia" target="_blank" rel="noopener" href="https://www.algolia.com/">algolia</a><a class="footer-item" title="twikoo" target="_blank" rel="noopener" href="https://twikoo.js.org/">twikoo</a><a class="footer-item" title="vercel" target="_blank" rel="noopener" href="https://vercel.com/">vercel</a></div></div><div class="footer-group"><div class="footer-title">主题</div><div class="footer-links"><a class="footer-item" title="文档" target="_blank" rel="noopener" href="https://docs.anheyu.com/">文档</a><a class="footer-item" title="源码" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu">源码</a><a class="footer-item" title="更新日志" target="_blank" rel="noopener" href="https://blog.anheyu.com/update/">更新日志</a></div></div><div class="footer-group"><div class="footer-title">导航</div><div class="footer-links"><a class="footer-item" title="即刻短文" href="/essay/">即刻短文</a><a class="footer-item" title="友链文章" href="/fcircle/">友链文章</a><a class="footer-item" title="留言板" href="/comments/">留言板</a></div></div><div class="footer-group"><div class="footer-title">协议</div><div class="footer-links"><a class="footer-item" title="隐私协议" href="/privacy/">隐私协议</a><a class="footer-item" title="Cookies" href="/cookies/">Cookies</a><a class="footer-item" title="版权协议" href="/copyright/">版权协议</a></div></div><div class="footer-group"><div class="footer-title-group"><div class="footer-title">友链</div><a class="random-friends-btn" id="footer-random-friends-btn" href="javascript:addFriendLinksInFooter();" title="换一批友情链接"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right"></i></a></div><div class="footer-links" id="friend-links-in-footer"></div></div></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v7.0.0" title="博客框架为Hexo_v7.0.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Frame-Hexo.svg" alt="博客框架为Hexo_v7.0.0"/></a><a class="github-badge" target="_blank" href="https://blog.anheyu.com/" style="margin-inline:5px" data-title="本站使用AnZhiYu主题" title="本站使用AnZhiYu主题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.9/img/Theme-AnZhiYu-2E67D3.svg" alt="本站使用AnZhiYu主题"/></a><a class="github-badge" target="_blank" href="https://www.dogecloud.com/" style="margin-inline:5px" data-title="本站使用多吉云为静态资源提供CDN加速" title="本站使用多吉云为静态资源提供CDN加速"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.2.0/img/badge/CDN-多吉云-3693F3.svg" alt="本站使用多吉云为静态资源提供CDN加速"/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title="本站项目由Github托管"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Source-Github.svg" alt="本站项目由Github托管"/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.2.0/img/badge/Copyright-BY-NC-SA.svg" alt="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"/></a></p></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2021 - 2024 By <a class="footer-bar-link" href="/" title="AYO" target="_blank">AYO</a></div></div><div id="footer-type-tips"></div><div class="js-pjax"><script>function subtitleType () {
  fetch('https://v1.hitokoto.cn')
    .then(response => response.json())
    .then(data => {
      if (true) {
        const from = '出自 ' + data.from
        const sub = ["生活明朗&#44; 万物可爱&#44; 人间值得&#44; 未来可期."]
        sub.unshift(data.hitokoto, from)
        window.typed = new Typed('#footer-type-tips', {
          strings: sub,
          startDelay: 300,
          typeSpeed: 150,
          loop: true,
          backSpeed: 50,
        })
      } else {
        document.getElementById('footer-type-tips').innerHTML = data.hitokoto
      }
    })
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.cbd.int/typed.js@2.0.15/dist/typed.umd.js').then(subtitleType)
  }
} else {
  subtitleType()
}
</script></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/AYO-Al/AYO-Al.github.io" title="博客">博客</a><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/AYO-Al" title="本站项目由GitHub托管">本站项目由GitHub托管</a><a class="footer-bar-link cc" href="/copyright" title="cc协议"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-by-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nc-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nd-line"></i></a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">53</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">46</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">8</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://git.yuki.love/" title="AYO网站"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="AYO网站"/><span class="back-menu-item-text">AYO网站</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://juejin.cn/user/3708003152300183" title="AYO博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="AYO博客"/><span class="back-menu-item-text">AYO博客</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://github.com/AYO-Al" title="GitHub"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="GitHub"/><span class="back-menu-item-text">GitHub</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/Ansible/" style="font-size: 0.88rem;">Ansible<sup>1</sup></a><a href="/tags/CDN/" style="font-size: 0.88rem;">CDN<sup>1</sup></a><a href="/tags/DNS/" style="font-size: 0.88rem;">DNS<sup>1</sup></a><a href="/tags/Database/" style="font-size: 0.88rem;">Database<sup>8</sup></a><a href="/tags/Docker/" style="font-size: 0.88rem;">Docker<sup>1</sup></a><a href="/tags/ELK/" style="font-size: 0.88rem;">ELK<sup>1</sup></a><a href="/tags/Flask/" style="font-size: 0.88rem;">Flask<sup>1</sup></a><a href="/tags/Httpd/" style="font-size: 0.88rem;">Httpd<sup>1</sup></a><a href="/tags/JavaScript/" style="font-size: 0.88rem;">JavaScript<sup>1</sup></a><a href="/tags/Keepalived/" style="font-size: 0.88rem;">Keepalived<sup>1</sup></a><a href="/tags/LVS/" style="font-size: 0.88rem;">LVS<sup>1</sup></a><a href="/tags/Linux/" style="font-size: 0.88rem;">Linux<sup>22</sup></a><a href="/tags/MLinux/" style="font-size: 0.88rem;">MLinux<sup>1</sup></a><a href="/tags/MySQL/" style="font-size: 0.88rem;">MySQL<sup>6</sup></a><a href="/tags/Nginx/" style="font-size: 0.88rem;">Nginx<sup>3</sup></a><a href="/tags/Python/" style="font-size: 0.88rem;">Python<sup>8</sup></a><a href="/tags/Rabbit/" style="font-size: 0.88rem;">Rabbit<sup>1</sup></a><a href="/tags/Redis/" style="font-size: 0.88rem;">Redis<sup>2</sup></a><a href="/tags/Shell/" style="font-size: 0.88rem;">Shell<sup>3</sup></a><a href="/tags/Web/" style="font-size: 0.88rem;">Web<sup>2</sup></a><a href="/tags/Zabbix/" style="font-size: 0.88rem;">Zabbix<sup>2</sup></a><a href="/tags/%E4%B8%89%E9%AB%98MySQL/" style="font-size: 0.88rem;">三高MySQL<sup>1</sup></a><a href="/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/" style="font-size: 0.88rem;">中间件<sup>1</sup></a><a href="/tags/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/" style="font-size: 0.88rem;">主从复制<sup>1</sup></a><a href="/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/" style="font-size: 0.88rem;">云计算<sup>7</sup></a><a href="/tags/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/" style="font-size: 0.88rem;">反向代理<sup>1</sup></a><a href="/tags/%E5%9B%9B%E5%A4%A7%E4%BB%B6/" style="font-size: 0.88rem;">四大件<sup>3</sup></a><a href="/tags/%E5%BC%80%E5%8F%91/" style="font-size: 0.88rem;">开发<sup>7</sup></a><a href="/tags/%E6%96%87%E4%BB%B6%E5%85%B1%E4%BA%AB%E6%9C%8D%E5%8A%A1/" style="font-size: 0.88rem;">文件共享服务<sup>1</sup></a><a href="/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/" style="font-size: 0.88rem;">文件系统<sup>1</sup></a><a href="/tags/%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" style="font-size: 0.88rem;">日志管理<sup>1</sup></a><a href="/tags/%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5/" style="font-size: 0.88rem;">时间同步<sup>1</sup></a><a href="/tags/%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/" style="font-size: 0.88rem;">服务管理<sup>1</sup></a><a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 0.88rem;">爬虫<sup>4</sup></a><a href="/tags/%E7%A3%81%E7%9B%98/" style="font-size: 0.88rem;">磁盘<sup>1</sup></a><a href="/tags/%E7%BD%91%E7%BB%9C/" style="font-size: 0.88rem;">网络<sup>4</sup></a><a href="/tags/%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/" style="font-size: 0.88rem;">网络架构<sup>1</sup></a><a href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4/" style="font-size: 0.88rem;">自动化运维<sup>1</sup></a><a href="/tags/%E8%BD%AF%E4%BB%B6%E7%AE%A1%E7%90%86/" style="font-size: 0.88rem;">软件管理<sup>1</sup></a><a href="/tags/%E9%98%B2%E7%81%AB%E5%A2%99/" style="font-size: 0.88rem;">防火墙<sup>1</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="anzhiyufont anzhiyu-icon-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random"></meting-js></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"><a class="tag-list" href="/tags/Linux" title="Linux">Linux</a><a class="tag-list" href="/tags/Database" title="Database">Database</a><a class="tag-list" href="/tags/Kubernetes" title="Kubernetes">Kubernetes</a></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.4/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2021 By 安知鱼 V1.6.10",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 AYO 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><script src="https://cdn.cbd.int/algoliasearch@4.18.0/dist/algoliasearch-lite.umd.js"></script><script src="https://cdn.cbd.int/instantsearch.js@4.56.5/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>(() => {
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.yuki.love',
      region: '',
      onCommentLoaded: () => {
        anzhiyu.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(runFn,0)
    else getScript('https://cdn.cbd.int/twikoo@1.6.25/dist/twikoo.all.min.js').then(runFn)
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo.yuki.love',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const runFn = () => {
    init();
    
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) anzhiyu.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else {
      loadTwikoo()
    }
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script><script>(() => {
  const initWaline = () => {
    const waline = Waline.init(Object.assign({
      el: '#waline-wrap',
      serverURL: '',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      path: window.location.pathname,
      comment: false,
    }, null))
  }

  const loadWaline = async () => {
    if (typeof Waline === 'object') initWaline()
    else {
      await getCSS('https://cdn.cbd.int/@waline/client@2.15.5/dist/waline.css')
      await getScript('https://cdn.cbd.int/@waline/client@2.15.5/dist/waline.js')
      initWaline()
    }
  }

  if ('Twikoo' === 'Waline' || !false) {
    if (false) anzhiyu.loadComment(document.getElementById('waline-wrap'),loadWaline)
    else setTimeout(loadWaline, 0)
  } else {
    window.loadOtherComment = loadWaline
  }
})()</script><input type="hidden" name="page-type" id="page-type" value="post"></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://twikoo.yuki.love',
        region: '',
        pageSize: 6,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.textContent= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.cbd.int/twikoo@1.6.25/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'><div class='name'><span>${array[i].nick} </span></div></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <time datetime="${array[i].date}">${anzhiyu.diffDate(array[i].date, true)}</time></div>
        </div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script>var visitorMail = "visitor@anheyu.com";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","meta[property=\"og:type\"]","meta[property=\"og:site_name\"]","meta[property=\"og:description\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>